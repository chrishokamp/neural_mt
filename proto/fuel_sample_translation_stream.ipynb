{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import codecs\n",
    "import tempfile\n",
    "import cPickle\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from picklable_itertools import iter_, chain\n",
    "from fuel.datasets import Dataset\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import numpy\n",
    "import pickle\n",
    "from subprocess import Popen, PIPE\n",
    "import codecs\n",
    "\n",
    "from blocks.algorithms import (GradientDescent, StepClipping,\n",
    "                               CompositeRule, Adam, AdaDelta)\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_noise, apply_dropout\n",
    "from blocks.initialization import IsotropicGaussian, Orthogonal, Constant\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.select import Selector\n",
    "from blocks.search import BeamSearch\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "\n",
    "from machine_translation.checkpoint import CheckpointNMT, LoadNMT\n",
    "from machine_translation.model import BidirectionalEncoder, Decoder\n",
    "from machine_translation.sampling import BleuValidator, Sampler, SamplingBase\n",
    "from machine_translation.stream import (get_tr_stream, get_dev_stream,\n",
    "                                        _ensure_special_tokens)\n",
    "\n",
    "\n",
    "from nnqe.dataset.preprocess import whitespace_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a fuel stream which subclasses text file to create a stream which provides three sources: \n",
    "# (source, [samples], and BLEU scores)\n",
    "\n",
    "# going forward, this may not be the fastest way because the sampling and BLEU score computation can be time consuming\n",
    "# we should look at Fuel's read-ahead and cacheing capacity\n",
    "\n",
    "# TODO: stateful transformer which takes a stream and adds the sources ('samples', 'scores')\n",
    "# class Mapping(Transformer)\n",
    "# the mapping should be a callable which gets samples, then computes the sentence-level BLEU\n",
    "# score for each sample with respect to the reference\n",
    "\n",
    "# use this script to get sentence-level scores(?)\n",
    "# https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\n",
    "\n",
    "# parameters\n",
    "# sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "# score_func: function\n",
    "\n",
    "# TODO: how do they create simple test streams in the fuel tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MTSampleStreamTransformer:\n",
    "    \"\"\"\n",
    "    Stateful transformer which takes a stream of (source, target) and adds the sources ('samples', 'scores')\n",
    "    \n",
    "    Samples are generated by calling the sample func with the source as argument\n",
    "    \n",
    "    Scores are generated by comparing each generated sample to the reference\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "    score_func: function\n",
    "\n",
    "    At call time, we expect a stream providing (sources,) -- i.e. something like a TextFile object\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, sample_func, score_func, num_samples=1):\n",
    "        self.sample_func = sample_func\n",
    "        self.score_func = score_func\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __call__(self, data):\n",
    "        print('calling transformer:')\n",
    "        source = data[0]\n",
    "        reference = data[1]\n",
    "        \n",
    "        print('num_samples: {}'.format(self.num_samples))\n",
    "        \n",
    "        # each sample may be of different length\n",
    "        samples = self.sample_func(numpy.array(source), self.num_samples)\n",
    "        scores = self._compute_scores(samples, reference)\n",
    "    \n",
    "        return (samples, scores)\n",
    "\n",
    "    # Note that sentence-level BLEU can be computed directly over the indexes (not the strings),\n",
    "    # So we don't need to map back to a string representation\n",
    "    def _compute_scores(self, samples, reference):\n",
    "        \"\"\"Call the scoring function to compare each sample to the reference\"\"\"\n",
    "        \n",
    "#         return numpy.ones(len(samples), dtype='float32')\n",
    "        return self.score_func(samples, reference)\n",
    "\n",
    "\n",
    "# def _get_true_length(seq, vocab):\n",
    "#     try:\n",
    "#         return seq.tolist().index(vocab['</S>']) + 1\n",
    "#     except ValueError:\n",
    "#         return len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fake_score(source, target):\n",
    "    return 1.\n",
    "\n",
    "def fake_sample(source, num_samples=1):\n",
    "    return numpy.vstack(list(itertools.repeat([2,67,33,778,323,68], num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_transformer = MTSampleStreamTransformer(fake_sample, fake_score, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling transformer:\n",
      "num_samples: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68]]), 1.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = ([[1,55,75,324,43,0], [6546,24,4123,73,85,13]])\n",
    "\n",
    "test_transformer(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating theano variables\n",
      "INFO:root:Building sampling model\n",
      "INFO:root:Creating Sampling Model...\n",
      "INFO:root:Loading parameters from model: /home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/backward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/back_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/back_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/back_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/back_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/forward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/fwd_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/fwd_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/fwd_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/fwd_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000, 300)   : /bidirectionalencoder/embeddings.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /decoder/sequencegenerator/att_trans/decoder.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/att_trans/attention/preprocess.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/att_trans/attention/preprocess.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/attention/state_trans/transform_states.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1)       : /decoder/sequencegenerator/att_trans/attention/energy_comp/linear.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 1600)   : /decoder/sequencegenerator/att_trans/distribute/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/readout/merge/transform_states.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000, 300)   : /decoder/sequencegenerator/readout/lookupfeedbackwmt15/lookuptable.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /decoder/sequencegenerator/readout/merge/transform_feedback.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/readout/merge/transform_weighted_averages.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/maxout_bias.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (400, 300)     : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax0.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 20000)   : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000,)       : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /decoder/sequencegenerator/fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /decoder/sequencegenerator/fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/decoder.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/att_trans/distribute/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /decoder/sequencegenerator/fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Number of parameters loaded for computation graph: 37\n"
     ]
    }
   ],
   "source": [
    "# create the graph which can sample from our model \n",
    "# Note that we must sample instead of getting the 1-best or N-best, because we need the randomness to make the expected\n",
    "# BLEU score make sense\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "    'saved_parameters': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz',\n",
    "    'src_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.en.pkl',\n",
    "    'trg_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.de.pkl',\n",
    "    'src_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.en.tok.shuf',\n",
    "    'trg_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.de.tok.shuf',\n",
    "    'unk_id':1\n",
    "}\n",
    "\n",
    "def get_sampling_model_and_input(exp_config):\n",
    "    # Create Theano variables\n",
    "    encoder = BidirectionalEncoder(\n",
    "        exp_config['src_vocab_size'], exp_config['enc_embed'], exp_config['enc_nhids'])\n",
    "\n",
    "    decoder = Decoder(\n",
    "        exp_config['trg_vocab_size'], exp_config['dec_embed'], exp_config['dec_nhids'],\n",
    "        exp_config['enc_nhids'] * 2)\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    sampling_input = tensor.lmatrix('source')\n",
    "\n",
    "    # Get beam search\n",
    "    logger.info(\"Building sampling model\")\n",
    "    sampling_representation = encoder.apply(\n",
    "        sampling_input, tensor.ones(sampling_input.shape))\n",
    "    generated = decoder.generate(sampling_input, sampling_representation)\n",
    "\n",
    "#     _, samples = VariableFilter(\n",
    "#         bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "#                  ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "#     beam_search = BeamSearch(samples=samples)\n",
    "\n",
    "    # build the model that will let us get a theano function from the sampling graph\n",
    "    logger.info(\"Creating Sampling Model...\")\n",
    "    sampling_model = Model(generated)\n",
    "\n",
    "    # Set the parameters from a trained models\n",
    "    logger.info(\"Loading parameters from model: {}\".format(exp_config['saved_parameters']))\n",
    "    # load the parameter values from an .npz file\n",
    "    param_values = LoadNMT.load_parameter_values(exp_config['saved_parameters'])\n",
    "    LoadNMT.set_model_parameters(sampling_model, param_values)\n",
    "    \n",
    "    return sampling_model, sampling_input\n",
    "\n",
    "test_model, theano_sampling_input = get_sampling_model_and_input(exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test that we can pull samples from the model\n",
    "test_sampling_func = test_model.get_theano_function()\n",
    "trg_vocab = cPickle.load(open(exp_config['trg_vocab']))\n",
    "trg_vocab_size = exp_config['trg_vocab_size'] - 1\n",
    "\n",
    "trg_vocab = _ensure_special_tokens(trg_vocab, bos_idx=0,\n",
    "                                   eos_idx=trg_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "\n",
    "\n",
    "# close over the sampling func and the trg_vocab to standardize the interface\n",
    "# TODO: actually this should be a callable class with params (sampling_func, trg_vocab)\n",
    "# TODO: we may be able to make this function faster by passing multiple sources for sampling at the same damn time\n",
    "def sampling_func(source_seq, num_samples=1):\n",
    "    print('sampling_func')\n",
    "    print('num_samples: {}'.format(num_samples))\n",
    "    \n",
    "    def _get_true_length(seq, trg_vocab):\n",
    "        try:\n",
    "            return seq.tolist().index(trg_vocab['</S>']) + 1\n",
    "        except ValueError:\n",
    "            return len(seq)\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        # outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "        #                 list(next_glimpses.values()) + [next_costs])\n",
    "        _1, outputs, _2, _3, costs = test_sampling_func(source_seq[None, :])\n",
    "        # if we are generating a single sample, the length of the output will be len(source_seq)*2\n",
    "        # see decoder.generate\n",
    "        # the output is a [seq_len, 1] array\n",
    "        outputs = outputs.reshape(outputs.shape[0])\n",
    "        outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        \n",
    "        samples.append(outputs)\n",
    "    \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make TextFile streams for source data and references, then merge them together\n",
    "\n",
    "def get_textfile_stream(source_file=None, src_vocab=None, src_vocab_size=30000,\n",
    "                      unk_id=1):\n",
    "    \"\"\"Create a TextFile dataset, and return a stream\"\"\"\n",
    "    \n",
    "    source_stream = None\n",
    "   \n",
    "    src_vocab = _ensure_special_tokens(\n",
    "        src_vocab if isinstance(src_vocab, dict) else\n",
    "        cPickle.load(open(src_vocab)),\n",
    "        bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)\n",
    "    source_dataset = TextFile([source_file], src_vocab, bos_token=None)\n",
    "    source_stream = DataStream(source_dataset)\n",
    "    return source_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_stream = get_textfile_stream(source_file=exp_config['src_data'], src_vocab=exp_config['src_vocab'],\n",
    "                                         src_vocab_size=exp_config['src_vocab_size'])\n",
    "\n",
    "# test_source_stream.sources = ('sources',)\n",
    "trg_stream = get_textfile_stream(source_file=exp_config['trg_data'], src_vocab=exp_config['trg_vocab'],\n",
    "                                         src_vocab_size=exp_config['trg_vocab_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge them to get a source, target pair\n",
    "training_stream = Merge([src_stream,\n",
    "                         trg_stream],\n",
    "                         ('source', 'target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampling_transformer = MTSampleStreamTransformer(sampling_func, fake_score, num_samples=5)\n",
    "\n",
    "training_stream = Mapping(training_stream, sampling_transformer, add_sources=('samples', 'scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling transformer:\n",
      "num_samples: 5\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "source: Two boys in blue shirts sit on the end of a bench next to a fair ride . </S>\n",
      "ref: Zwei Jungen in blauen T-Shirts sitzen am Ende einer Bank neben einem Fahrgeschäft . </S>\n",
      "sample: Zwei Jungen in Bluejeans sitzen am Ende einer Bank neben einem Jahrmarkt . </S>\n",
      "len sample: 14\n",
      "sample: Zwei Jungen in blauen Hemden sitzen am Ende einer Treppe neben einem Jahrmarkt . </S>\n",
      "len sample: 15\n",
      "sample: Zwei Jungen in blauen Hemden sitzen am Ende einer Holzbank und neben einem Jahrmarkt . </S>\n",
      "len sample: 16\n",
      "sample: Zwei Jungen in blauen Hemden sitzen auf einem hergerichteten Schale neben einem Jahrmarkt . </S>\n",
      "len sample: 15\n",
      "sample: Zwei Jungen mit blauen Hemden sitzen am Ende einer Bank neben einem Fahrgeschäft in der Stadt . </S>\n",
      "len sample: 18\n",
      "scores: 1.0\n",
      "calling transformer:\n",
      "num_samples: 5\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "source: A man pointing to the audience area on stage while holding a microphone with banjos behind him . </S>\n",
      "ref: Ein Mann auf einer Bühne zeigt ins Publikum , hält ein Mikrofon und hinter ihm befinden sich Banjospieler . </S>\n",
      "sample: Ein Mann zeigt auf die Bühne seines Gesichts ruht sich auf auf und ein Mikrofon an ist an der Bühne untergeht ist ein Mikrofon zu sehen sind . </S>\n",
      "len sample: 29\n",
      "sample: Ein steht zeigt auf der Bühne von Leuten die Gitarre der Banduniformen und hinter ihm LeBron Schwimmbeckens . </S>\n",
      "len sample: 19\n",
      "sample: Ein Mann zeigt auf der Bühne auf die Schwimmbeckens und ist ein Mikrofon mit Baumstämmen angelehnt Tech „ real clear “ bewachsen ist . </S>\n",
      "len sample: 25\n",
      "sample: Ein Mann zeigt auf die Bühne auf die Seiten der Bühne fest , während ein Zeichen auf ihm Gießen hergeht . </S>\n",
      "len sample: 22\n",
      "sample: Ein Mann zeigt auf die Bühne inmitten von Bäumen im flachen Luftballons stehen , zurück und hinter ihm bückt sich ein Mikrofon . </S>\n",
      "len sample: 24\n",
      "scores: 1.0\n",
      "calling transformer:\n",
      "num_samples: 5\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "source: A little girl riding a two wheeler , wearing her helmet close to the street with passing cars . </S>\n",
      "ref: Ein kleines Mädchen mit Helm auf einem Zweirad nahe der Straße mit fahrenden Autos . </S>\n",
      "sample: Ein kleines Mädchen fährt mit Helm Sandalen , der neben der Straße mit Schildern Kopftüchern im Stadtpark ist . </S>\n",
      "len sample: 20\n",
      "sample: Ein kleines Mädchen fährt bei einem Innenhof zwei Stangen hindurch , der unter Tüten clear Ford ist , verdeckt sind </S>\n",
      "len sample: 21\n",
      "sample: Ein kleines Mädchen fährt mit einem Stangen Funken fliegen neben den vorne in der Nähe der Straße neben Schildern mit Schildern . </S>\n",
      "len sample: 23\n",
      "sample: Ein kleines Mädchen fährt auf einem zwei Fahrräder , der die Helme tragen , sind neben der Straße mit Schildern eines Waldes . </S>\n",
      "len sample: 24\n",
      "sample: Ein kleines Mädchen fahren auf einem Quad durch der Nähe von Schildern mit geparkten Autos . </S>\n",
      "len sample: 17\n",
      "scores: 1.0\n",
      "calling transformer:\n",
      "num_samples: 5\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "source: A road worker in green , there is an orange cone near him . </S>\n",
      "ref: Ein Straßenarbeiter in grüner Kleidung , in seiner Nähe ein oranger Kegel . </S>\n",
      "sample: Ein Straßenarbeiter in grün ist ist auf einem Fußgängerübergang . </S>\n",
      "len sample: 11\n",
      "sample: Ein der Straße in Grün und der eine orange Waffel . </S>\n",
      "len sample: 12\n",
      "sample: Ein Straßenarbeiter in Grün . </S>\n",
      "len sample: 6\n",
      "sample: Ein Straße in Grün und einem orange- Körper in der Nähe ihm . </S>\n",
      "len sample: 14\n",
      "sample: Ein Straße in grüner Bekleidung und eine orange Waffel . </S>\n",
      "len sample: 11\n",
      "scores: 1.0\n",
      "calling transformer:\n",
      "num_samples: 5\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "source: A man leans against a building near a busy street . </S>\n",
      "ref: Ein Mann lehnt sich in der Nähe einer belebten Straße an ein Gebäude . </S>\n",
      "sample: Ein Mann lehnt an einem Gebäude neben einer stark befahrenen Straße . </S>\n",
      "len sample: 13\n",
      "sample: Ein Mann lehnt an einem Gebäude neben einer stark befahrenen Straße . </S>\n",
      "len sample: 13\n",
      "sample: Ein Mann lehnt sich an einem Gebäude an der Straße an einem Gebäude . </S>\n",
      "len sample: 15\n",
      "sample: Ein Mann lehnt auf einem Gebäude neben einer stark befahrenen Straße . </S>\n",
      "len sample: 13\n",
      "sample: Ein Mann lehnt sich an einem Gebäude an einem Gebäude . </S>\n",
      "len sample: 12\n",
      "scores: 1.0\n"
     ]
    }
   ],
   "source": [
    "# SANITY\n",
    "# load src and target vocabs to validate samples\n",
    "src_vocab = cPickle.load(open(exp_config['src_vocab']))\n",
    "src_vocab_size = exp_config['src_vocab_size'] - 1\n",
    "trg_vocab_size = exp_config['trg_vocab_size'] - 1\n",
    "\n",
    "\n",
    "src_vocab = _ensure_special_tokens(src_vocab, bos_idx=0,\n",
    "                                   eos_idx=src_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "\n",
    "src_ivocab = {v:k for k,v in src_vocab.items()}\n",
    "trg_ivocab = {v:k for k,v in trg_vocab.items()}\n",
    "\n",
    "k = 5\n",
    "test_iter = training_stream.get_epoch_iterator()\n",
    "for _ in range(k):\n",
    "    source, reference, samples, scores = test_iter.next()\n",
    "    print('source: {}'.format(' '.join(src_ivocab[w] for w in source)))\n",
    "    print('ref: {}'.format(' '.join(trg_ivocab[w] for w in reference)))\n",
    "    for sample in samples:\n",
    "        print('sample: {}'.format(' '.join(trg_ivocab[w] for w in sample)))\n",
    "        print('len sample: {}'.format(len(sample)))\n",
    "    print('scores: {}'.format(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now transform the training stream into a stream providing (sources, samples, scores)\n",
    "# sources is tensor.lmatrix\n",
    "# targets is tensor.tensor3 - dtype int64 (N target samples for each source)\n",
    "# scores is tensor.matrix - dtype float32 -- each row contains the N scores for the N samples\n",
    "# generated by the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calling the external scoring script\n",
    "sentence_level_bleu_script = '/home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl'\n",
    "\n",
    "#     self.multibleu_cmd = ['perl', self.config['bleu_script'],\n",
    "#                               self.config['val_set_grndtruth'], '<']\n",
    "    \n",
    "    \n",
    "# make temporary files for source, targets, references (sources are required by mteval-v13a.pl)\n",
    "# TODO: automatically make temporary files that have this data wrapped in SGML so that it can work with moses\n",
    "# NOTE: remember the sentence level BLEU score python interface in CDEC that we used for QE 2014, \n",
    "test_hyps='/home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.target'\n",
    "test_refs='/home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.ref'\n",
    "test_srcs='/home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.source'\n",
    "# test_hyps='/home/chris/projects/neural_mt/scripts/scoring/test.target.de.sgm'\n",
    "# test_refs='/home/chris/projects/neural_mt/scripts/scoring/test.ref.de.sgm'\n",
    "# test_srcs='/home/chris/projects/neural_mt/scripts/scoring/test.source.en.sgm'\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "  \"Description:  This Perl script evaluates MT system performance.\\n\".\n",
    "    \"\\n\".\n",
    "    \"Required arguments:\\n\".\n",
    "    \"  -r <ref_file> is a file containing the reference translations for\\n\".\n",
    "    \"      the documents to be evaluated.\\n\".\n",
    "    \"  -s <src_file> is a file containing the source documents for which\\n\".\n",
    "    \"      translations are to be evaluated\\n\".\n",
    "    \"  -t <tst_file> is a file containing the translations to be evaluated\\n\".\n",
    "    \"\\n\".\n",
    "    \"Optional arguments:\\n\".\n",
    "    \"  -h prints this help message to STDOUT\\n\".\n",
    "    \"  -c preserves upper-case alphabetic characters\\n\".\n",
    "    \"  -b generate BLEU scores only\\n\".\n",
    "    \"  -n generate NIST scores only\\n\".\n",
    "    \"  -d detailed output flag:\\n\".\n",
    "    \"         0 (default) for system-level score only\\n\".\n",
    "    \"         1 to include document-level scores\\n\".\n",
    "    \"         2 to include segment-level scores\\n\".\n",
    "    \"         3 to include ngram-level scores\\n\".\n",
    "    \"  -e enclose non-ASCII characters between spaces\\n\".\n",
    "     \"  --brevity-penalty ( closest | shortest )\\n\" .\n",
    "    \"         closest (default) : acts as IBM BLEU (takes the closest reference translation length)\\n\" .\n",
    "    \"         shortest : acts as previous versions of the script (takes the shortest reference translation length)\\n\" .\n",
    "    \"  --international-tokenization\\n\" .\n",
    "    \"         when specified, uses Unicode-based (only) tokenization rules\\n\" .\n",
    "    \"         when not specified (default), uses default tokenization (some language-dependant rules)\\n\" .\n",
    "    \"  --metricsMATR : create three files for both BLEU scores and NIST scores:\\n\" .\n",
    "    \"         BLEU-seg.scr and NIST-seg.scr : segment-level scores\\n\" .\n",
    "    \"         BLEU-doc.scr and NIST-doc.scr : document-level scores\\n\" .\n",
    "    \"         BLEU-sys.scr and NIST-sys.scr : system-level scores\\n\" .\n",
    "    \"  --no-smoothing : disable smoothing on BLEU scores\\n\" .\n",
    "    \"\\n\";\n",
    "\"\"\"\n",
    "\n",
    "# '--international-tokenization' ??\n",
    "# '-c preserve case' ??\n",
    "sentence_bleu_cmd = ['perl', sentence_level_bleu_script, '-b', '-r', test_refs, '-s', test_srcs, '-t', test_hyps, \n",
    "                     '-d', str(2)]\n",
    "\n",
    "\n",
    "bleu_score_subprocess = Popen(sentence_bleu_cmd, stdout=PIPE)\n",
    "# bleu_score_subprocess.stdin.flush()\n",
    "\n",
    "subprocess.check_output(sentence_bleu_cmd, shell=True)\n",
    "\n",
    "# bleu_out, bleu_err = bleu_score_subprocess.communicate()\n",
    "\n",
    "\n",
    "#      # Write to subprocess and file if it exists\n",
    "#     print(trans_out, file=mb_subprocess.stdin)\n",
    "            \n",
    "            \n",
    "#         # send end of file, read output.\n",
    "#         mb_subprocess.stdin.close()\n",
    "#         stdout = mb_subprocess.stdout.readline()\n",
    "#         logger.info(stdout)\n",
    "#         out_parse = re.match(r'BLEU = [-.0-9]+', stdout)\n",
    "#         logger.info(\"Validation Took: {} minutes\".format(\n",
    "#             float(time.time() - val_start_time) / 60.))\n",
    "#         assert out_parse is not None\n",
    "\n",
    "#         # extract the score\n",
    "#         bleu_score = float(out_parse.group()[6:])\n",
    "#         self.val_bleu_curve.append(bleu_score)\n",
    "#         logger.info(bleu_score)\n",
    "#         mb_subprocess.terminate()\n",
    "\n",
    "\n",
    "#         return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first wrap the source\n",
    "# perl wrap-xml-modified.perl en ../../proto/data_for_testing_sampling/test.source unbabel src < ../../proto/data_for_testing_sampling/test.source > test.source.en.sgm\n",
    "# now wrap target and ref\n",
    "# target hyps\n",
    "# perl wrap-xml-modified.perl de test.source.en.sgm unbabel tst < ../../proto/data_for_testing_sampling/test.target > test.target.de.sgm\n",
    "# target refs\n",
    "# perl wrap-xml-modified.perl de test.source.en.sgm unbabel ref < ../../proto/data_for_testing_sampling/test.ref > test.ref.de.sgm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from Unbabel evaluation\n",
    "def wrap_xml(fname, language=None, src_lang=None, flag='tst', engine='moses'):\n",
    "    '''\n",
    "    :param fname: The filename that needs to be wrapped around xml tags.\n",
    "                  The type can be taken from the file's suffix\n",
    "    :param engine: The MT engine the decoded text comes from\n",
    "    :param flag: string - source, reference, target (src, ref, tst)\n",
    "    :param language: string - the language of the text\n",
    "    :return: fname.flag.language.sgm that is the input file ins SGML format\n",
    "    '''\n",
    "    logger.info(\"Wrapping xml for %s with flag %s\" % (fname, flag))\n",
    "    if flag == 'src':\n",
    "        \"\"\" As an exception, if there is no source for reference,\n",
    "        we pass the same file name \"\"\"\n",
    "        source_sgml = fname\n",
    "    elif flag == 'ref':\n",
    "        base_fname = '.'.join(fname.split('.')[:-1])\n",
    "        source_sgml = \"%s.%s.sgm\" % (base_fname, src_lang)\n",
    "    else:\n",
    "        source_sgml = fname.replace(engine, src_lang) + '.sgm'\n",
    "\n",
    "    wrap_xml_script = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'scripts',\n",
    "                                   'wrap-xml-modified.perl')\n",
    "    # perl ../wrap-xml-modified.perl en 10.unbabel201410.es-en.source.sgm bing tst < 10.unbabel201410.es-en.bing\n",
    "    # > 10.unbabel201410.es-en.bing.sgm\n",
    "    wrap_xml_cmd = ['perl', wrap_xml_script, language, source_sgml, engine, flag]\n",
    "    logger.debug(' '.join(wrap_xml_cmd) + '< ' + fname)\n",
    "    with open(fname, 'r+') as f:\n",
    "        # r.readall() gave an error, this is a hack\n",
    "        fname_stdin = ''.join(f.readlines())\n",
    "\n",
    "    with open(fname + '.sgm', 'w+') as f:\n",
    "        p = subprocess.Popen(wrap_xml_cmd, stdout=f, stdin=subprocess.PIPE)\n",
    "        p.communicate(input=fname_stdin)[0]\n",
    "\n",
    "\n",
    "\n",
    "def mteval_13(source_file, reference_file, hypothesis_file, src_lang='en', trg_lang='de', engine='nmt'):\n",
    "    '''\n",
    "    Calling WMT Perl script for the evaluation\n",
    "    :param source_file: source SGML file\n",
    "    :param reference_file: reference SGML file\n",
    "    :param decoded_file: mt-translated SGML file\n",
    "    :return: list of sentence level BLEU for each segment\n",
    "    '''\n",
    "    \n",
    "    #     WORKING: incorporate SGML wrapping into temp files and BLEU score computation all into this script\n",
    "    #     make named temporary files and use these to hold wrap-xml output, delete them after\n",
    "\n",
    "    wrap_xml_script = '/home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl'\n",
    "    \n",
    "    flags = ['src', 'ref', 'tst']\n",
    "    inps_and_flags = zip([source_file, reference_file, hypothesis_file], flags)\n",
    "    \n",
    "    wrapped_files = []\n",
    "    for filename, flag in inps_and_flags:\n",
    "        # the following logic assumes 'src' is the first item in the list, otherwise it will break\n",
    "        if flag == 'src':\n",
    "            \"\"\"For the source, just pass the source filename\"\"\"\n",
    "            source_sgml = filename\n",
    "            # create the named temporary file that will hold the output sgml file\n",
    "            source_sgm_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "            source_sgm_name = source_sgm_file.name\n",
    "            output_sgm_name = source_sgm_name\n",
    "            language = src_lang\n",
    "        \n",
    "         # TODO: why were they providing different sgml sources for ref and target?\n",
    "        elif flag == 'ref':\n",
    "#             base_fname = '.'.join(fname.split('.')[:-1])\n",
    "#         source_sgml = \"%s.%s.sgm\" % (base_fname, src_lang)\n",
    "            source_sgml = source_sgm_name\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "            output_sgm_name = temp_file.name\n",
    "            ref_sgm_name = output_sgm_name\n",
    "            language = trg_lang\n",
    "\n",
    "        else:\n",
    "            source_sgml = source_sgm_name\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "            output_sgm_name = temp_file.name\n",
    "            hyp_sgm_file = output_sgm_name\n",
    "            language = trg_lang\n",
    "            \n",
    "#         write the new sgm file\n",
    "        wrap_xml_cmd = ['perl', wrap_xml_script, language, source_sgml, engine, flag]\n",
    "        logger.debug(' '.join(wrap_xml_cmd) + '< ' + filename)\n",
    "        with open(filename, 'r+') as f:\n",
    "            fname_stdin = f.read()\n",
    "\n",
    "        with open(output_sgm_name, 'w+') as f:\n",
    "            p = subprocess.Popen(wrap_xml_cmd, stdout=f, stdin=subprocess.PIPE)\n",
    "            p.communicate(input=fname_stdin)[0]\n",
    "    \n",
    "    # now compute the segment-level BLEU scores\n",
    "    mteval_2013_script = '/home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl'\n",
    "#     mteval_2013_script = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'scripts', 'mteval-v13a.pl')\n",
    "#     mteval_cmd = ['perl', mteval_2013_script, '-r', reference_file, '-s',\n",
    "#                   source_file, '-t', decoded_file, '-b', '-d', '2']\n",
    "    mteval_cmd = ['perl', mteval_2013_script, '-r', ref_sgm_name, '-s',\n",
    "                  source_sgm_name, '-t', hyp_sgm_file, '-b', '-d', '2']\n",
    "    logger.debug(' '.join(mteval_cmd))\n",
    "    mteval_proc = subprocess.Popen(mteval_cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE,\n",
    "                                   stderr=subprocess.PIPE)\n",
    "    stdout_data = mteval_proc.communicate()[0]\n",
    "    \n",
    "    # now parse stdout_data to get the scores and do asserts to make sure parsing is correct\n",
    "    per_seg_lines = stdout_data.split('\\n\\n')[1]\n",
    "    per_seg_lines = per_seg_lines.split('\\n')\n",
    "        \n",
    "    # the last two lines are corpus-level info\n",
    "    per_seg_lines = per_seg_lines[:-2]\n",
    "    bleu_scores = [l.split()[5] for l in per_seg_lines]\n",
    "    pprint(bleu_scores)\n",
    "    \n",
    "    with open(hypothesis_file) as inp:\n",
    "        # the substring we're going to count -- we need to do it like this because the\n",
    "        # moses wrap-xml script adds lines, but each real line is wrapped in <seg id=...></seg> tags \n",
    "#         line_substring = 'seg id='\n",
    "        num_segments = inp.read().strip().split('\\n')\n",
    "    print(num_segments)\n",
    "    print(len(bleu_scores))\n",
    "            \n",
    "    assert len(bleu_scores) == num_segments, \"We must get one score for each segment\"\n",
    "    \n",
    "    return bleu_scores\n",
    "\n",
    "\n",
    "#     newfile.write(\"\\n\".join([\"\\t\".join(lines)\n",
    "#                              for lines in map(list, zip(*all_lines))]).encode(\"utf-8\"))\n",
    "#     newfile.flush()\n",
    "#     temp_file_name = newfile.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.source nmt src< /home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.source\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpbbu9C3 nmt ref< /home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.ref\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpbbu9C3 nmt tst< /home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.target\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpsdKgbb -s /tmp/tmpbbu9C3 -t /tmp/tmpiGv7BJ -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000',\n",
      " '1.0000']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "We must get one score for each segment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-d584f91614f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmteval_13\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_srcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_refs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_hyps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-61fade294bcc>\u001b[0m in \u001b[0;36mmteval_13\u001b[1;34m(source_file, reference_file, hypothesis_file, src_lang, trg_lang, engine)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mnum_segments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbleu_scores\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnum_segments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"We must get one score for each segment\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbleu_scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: We must get one score for each segment"
     ]
    }
   ],
   "source": [
    "mteval_13(test_srcs, test_refs, test_hyps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/chris/projects/neural_mt/proto/data_for_testing_sampling/test.target'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hyps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bleu_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mteval script output \n",
    "(1:2008)$ perl mteval-v13a.pl -b -d 2 -r test.ref.de.sgm -s test.source.en.sgm -t test.target.de.sgm \n",
    "Use of 'Hyphen' in \\p{} or \\P{} is deprecated because: Supplanted by Line_Break property values; see www.unicode.org/reports/tr14; at mteval-v13a.pl line 951.\n",
    "MT evaluation scorer began on 2016 Mar 11 at 17:32:48\n",
    "command line:  mteval-v13a.pl -b -d 2 -r test.ref.de.sgm -s test.source.en.sgm -t test.target.de.sgm\n",
    "  Evaluation of any-to-de translation using:\n",
    "    src set \"SETID\" (1 docs, 20 segs)\n",
    "    ref set \"SETID\" (1 refs)\n",
    "    tst set \"SETID\" (1 systems)\n",
    "\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 1 of document \"../../proto/data_for_testing_sampling/test.source\" (14 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 2 of document \"../../proto/data_for_testing_sampling/test.source\" (19 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 3 of document \"../../proto/data_for_testing_sampling/test.source\" (15 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 4 of document \"../../proto/data_for_testing_sampling/test.source\" (13 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 5 of document \"../../proto/data_for_testing_sampling/test.source\" (14 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 6 of document \"../../proto/data_for_testing_sampling/test.source\" (14 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 7 of document \"../../proto/data_for_testing_sampling/test.source\" (10 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 8 of document \"../../proto/data_for_testing_sampling/test.source\" (9 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 9 of document \"../../proto/data_for_testing_sampling/test.source\" (13 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 10 of document \"../../proto/data_for_testing_sampling/test.source\" (13 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 11 of document \"../../proto/data_for_testing_sampling/test.source\" (8 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 12 of document \"../../proto/data_for_testing_sampling/test.source\" (11 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 13 of document \"../../proto/data_for_testing_sampling/test.source\" (11 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 14 of document \"../../proto/data_for_testing_sampling/test.source\" (12 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 15 of document \"../../proto/data_for_testing_sampling/test.source\" (11 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 16 of document \"../../proto/data_for_testing_sampling/test.source\" (11 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 17 of document \"../../proto/data_for_testing_sampling/test.source\" (16 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 18 of document \"../../proto/data_for_testing_sampling/test.source\" (13 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 19 of document \"../../proto/data_for_testing_sampling/test.source\" (14 words)\n",
    "  BLEU score using 4-grams = 1.0000 for system \"unbabel\" on segment 20 of document \"../../proto/data_for_testing_sampling/test.source\" (9 words)\n",
    "BLEU score using   4-grams = 1.0000 for system \"unbabel\" on document \"../../proto/data_for_testing_sampling/test.source\" (20 segments, 250 words)\n",
    "length ratio: 1 (250/250), penalty (log): 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model related -----------------------------------------------------------\n",
    "# Sequences longer than this will be discarded\n",
    "'seq_len': 40\n",
    "# Number of hidden units in encoder/decoder GRU\n",
    "'enc_nhids': &REC_SIZE 800\n",
    "'dec_nhids': 800\n",
    "\n",
    "# Dimension of the word embedding matrix in encoder/decoder\n",
    "'enc_embed': &EMBED_SIZE 300\n",
    "'dec_embed': 300\n",
    "\n",
    "# Optimization related ----------------------------------------------------\n",
    "# Batch size\n",
    "'batch_size': &BATCH_SIZE 15 \n",
    "\n",
    "# This many batches will be read ahead and sorted\n",
    "'sort_k_batches': 15 \n",
    "\n",
    "# Optimization step rule\n",
    "'step_rule': &STEP_RULE 'Adam'\n",
    "\n",
    "# Gradient clipping threshold\n",
    "'step_clipping': 1.\n",
    "\n",
    "# Std of weight initialization\n",
    "'weight_scale': 0.01\n",
    "\n",
    "# Regularization related --------------------------------------------------\n",
    "\n",
    "# Weight noise flag for feed forward layers\n",
    "'weight_noise_ff': &FF_NOISE False\n",
    "\n",
    "# Weight noise flag for recurrent layers\n",
    "'weight_noise_rec': False\n",
    "\n",
    "# Dropout ratio, applied only after readout maxout\n",
    "'dropout': &DROPOUT 0.3\n",
    "\n",
    "# Source and target vocabulary sizes, should include bos, eos, unk tokens\n",
    "'src_vocab_size': &SRC_VOCAB_SIZE 20000\n",
    "'trg_vocab_size': &TGT_VOCAB_SIZE 20000\n",
    "\n",
    "# Special tokens and indexes\n",
    "'unk_id': 1\n",
    "'bos_token': '<S>'\n",
    "'eos_token': '</S>'\n",
    "'unk_token': '<UNK>'\n",
    "\n",
    "# Root directory for dataset\n",
    "'datadir': &DATADIR /media/1tb_drive/multilingual-multimodal/flickr30k/train/processed\n",
    "\n",
    "# the name of the directory where the model will be saved and checkpointed\n",
    "#'model_save_directory': &SAVEDIR !format_str ['unbabel_data_dropout{}_ff_noise{}_search_model_en2es_vocab{}_emb{}_rec{}_batch{}', *DROPOUT, *FF_NOISE, *SRC_VOCAB_SIZE, *EMBED_SIZE, *REC_SIZE, *BATCH_SIZE]\n",
    "'model_save_directory': &SAVEDIR !format_str ['BERTHA-TEST_{}_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15', *STEP_RULE]\n",
    "\n",
    "# Where to save model, this corresponds to 'prefix' in groundhog\n",
    "'saveto': &OUTPUT_DIR !path_join [*DATADIR, *SAVEDIR]\n",
    "\n",
    "# Module name of the stream that will be used\n",
    "# note this requires the stream to be implemented as a module -- there may be a better way\n",
    "'stream': 'stream'\n",
    "\n",
    "# Source and target vocabularies\n",
    "'src_vocab': !path_join [*DATADIR, 'vocab.en-de.en.pkl']\n",
    "'trg_vocab': !path_join [*DATADIR, 'vocab.en-de.de.pkl']\n",
    "\n",
    "# Source and target datasets\n",
    "'src_data': !path_join [*DATADIR, 'train.en.tok.shuf']\n",
    "'trg_data': !path_join [*DATADIR, 'train.de.tok.shuf']\n",
    "\n",
    "\n",
    "# Early stopping based on BLEU score on dev set ------------------------------------\n",
    "\n",
    "# Normalize cost according to sequence length after beam-search\n",
    "'normalized_bleu': True\n",
    "\n",
    "# Bleu script that will be used (moses multi-perl in this case)\n",
    "'bleu_script': !path_join [*DATADIR, 'multi-bleu.perl']\n",
    "\n",
    "# Validation set source file\n",
    "'val_set': !path_join [*DATADIR, 'dev.en.tok']\n",
    "\n",
    "# Validation set gold file\n",
    "'val_set_grndtruth': !path_join [*DATADIR, 'dev.de.tok']\n",
    "\n",
    "# Print validation output to file\n",
    "'output_val_set': True\n",
    "\n",
    "# Validation output file\n",
    "'val_set_out': !path_join [*OUTPUT_DIR, 'validation_out.txt']\n",
    "\n",
    "# Beam-size\n",
    "'beam_size': 20 \n",
    "\n",
    "# Timing/monitoring related -----------------------------------------------\n",
    "\n",
    "# Maximum number of updates\n",
    "'finish_after': 1000000\n",
    "\n",
    "# Reload model from files if exist\n",
    "'reload': True\n",
    "\n",
    "# Save model after this many updates\n",
    "'save_freq': 5000\n",
    "\n",
    "# Show samples from model after this many updates\n",
    "'sampling_freq': 5000\n",
    "\n",
    "# Show this many samples at each sampling\n",
    "'hook_samples': 5\n",
    "\n",
    "# Validate bleu after this many updates\n",
    "'bleu_val_freq': 1000 \n",
    "\n",
    "# Start bleu validation after this many updates\n",
    "'val_burn_in': 5000\n",
    "\n",
    "# Using trained models for prediction ------------\n",
    "\n",
    "# The location of the saved parameters of a trained model as .npz\n",
    "'saved_parameters': ~\n",
    "\n",
    "# The location of a test set in the source language\n",
    "'test_set': ~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MTSampleStreamGenerator(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_files: list[str] - the files containing the source seqs we'll use to generate samples\n",
    "    sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "    score_func: function\n",
    "    \n",
    "    # source dict only or source + target dict?\n",
    "    dictionary: dict mapping word-->id -- TODO: switch to parameterized choice\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    provides_sources = ('sources','targets', 'scores')\n",
    "    example_iteration_scheme = None\n",
    "\n",
    "    def __init__(self, files, dictionary, all_idxs_and_probs, bos_token=None, eos_token=None,\n",
    "                 unk_token='<UNK>', level='word', preprocess=None):\n",
    "        self.files = files\n",
    "        self.dictionary = dictionary\n",
    "        # use all idxs to choose random substitutions and insertions\n",
    "        # WORKING: switch to parameterized choice using word freq\n",
    "        # WORKING pass in two lists -- words and probs (use the p= kwarg from np.random.choice)\n",
    "        # self.all_idxs = dictionary.values()\n",
    "        self.all_idxs, self.all_probs = zip(*all_idxs_and_probs.items())\n",
    "        if bos_token is not None and bos_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.bos_token = bos_token\n",
    "        if eos_token is not None and eos_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.eos_token = eos_token\n",
    "        if unk_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.unk_token = unk_token\n",
    "        if level not in ('word', 'character'):\n",
    "            raise ValueError\n",
    "        self.level = level\n",
    "        self.preprocess = preprocess\n",
    "        super(RandomTargetGenerator, self).__init__()\n",
    "\n",
    "    def open(self):\n",
    "        return chain(*[iter_(open(f)) for f in self.files])\n",
    "\n",
    "    def get_data(self, state=None, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        sentence = next(state)\n",
    "        if self.preprocess is not None:\n",
    "            sentence = self.preprocess(sentence)\n",
    "        pe_sequence = [self.dictionary[self.bos_token]] if self.bos_token else []\n",
    "        if self.level == 'word':\n",
    "            pe_sequence.extend(self.dictionary.get(word,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for word in sentence.split())\n",
    "        else:\n",
    "            pe_sequence.extend(self.dictionary.get(char,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for char in sentence.strip())\n",
    "        if self.eos_token:\n",
    "            pe_sequence.append(self.dictionary[self.eos_token])\n",
    "\n",
    "        # now use the pe sequence to generate artificial target_sequence and tag_sequence\n",
    "        target_sequence, tag_sequence = self._generate_artificial_target_and_tag_seqs(pe_sequence)\n",
    "\n",
    "        return (target_sequence, tag_sequence)\n",
    "\n",
    "    # TODO: currently only supports subs, not inserts\n",
    "    # TODO: tag identities are hard-coded as 0=BAD 1=OK\n",
    "    def _generate_artificial_target_and_tag_seqs(self, pe_sequence):\n",
    "        target_sequence = copy.copy(pe_sequence)\n",
    "        tag_sequence = [1] * len(pe_sequence)\n",
    "\n",
    "        # TODO: make num_subs distribution configurable\n",
    "        num_subs_to_make = numpy.random.randint(0, (int(numpy.ceil(len(pe_sequence / 1.3)))))\n",
    "        sub_idxs = numpy.random.choice(len(pe_sequence), size=num_subs_to_make)\n",
    "#         print('len orig seq: {}'.format(len(pe_sequence)))\n",
    "#         print('len orig seq / 2: {}'.format(len(pe_sequence) //2))\n",
    "#         print('n_subs: {}'.format(num_subs_to_make))\n",
    "#         print(sub_idxs)\n",
    "        for idx in sub_idxs:\n",
    "            new_word = numpy.random.choice(self.all_idxs, p=self.all_probs)\n",
    "            target_sequence[idx] = new_word\n",
    "            tag_sequence[idx] = 0\n",
    "\n",
    "        return target_sequence, tag_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
