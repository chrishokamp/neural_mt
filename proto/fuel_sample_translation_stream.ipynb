{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import codecs\n",
    "import tempfile\n",
    "import cPickle\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from picklable_itertools import iter_, chain\n",
    "from fuel.datasets import Dataset\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.transformers import Merge\n",
    "\n",
    "from nnqe.dataset.preprocess import whitespace_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a fuel stream which subclasses text file to create a stream which provides three sources: \n",
    "# (source, [samples], and BLEU scores)\n",
    "\n",
    "# going forward, this may not be the fastest way because the sampling and BLEU score computation can be time consuming\n",
    "# we should look at Fuel's read-ahead and cacheing capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: stateful transformer which takes a stream and adds the sources ('samples', 'scores')\n",
    "# class Mapping(Transformer)\n",
    "# the mapping should be a callable which gets samples, then computes the sentence-level BLEU\n",
    "# score for each sample with respect to the reference\n",
    "\n",
    "# use this script to get sentence-level scores(?)\n",
    "# https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\n",
    "\n",
    "# parameters\n",
    "# sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "# score_func: function\n",
    "\n",
    "# TODO: how do they create simple test streams in the fuel tests\n",
    "\n",
    "\n",
    "class MTSampleStreamTransformer(object):\n",
    "    \"\"\"\n",
    "    Stateful transformer which takes a stream and adds the sources ('samples', 'scores')\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "    score_func: function\n",
    "\n",
    "    At call time, we expect a stream providing (sources,) -- i.e. something like a TextFile object\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, sample_func, score_func):\n",
    "        self.sample_func = sample_func\n",
    "        self.score_func = score_func\n",
    "\n",
    "    def __call__(self, source_data):\n",
    "        return ([x for x in source_data[0]])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fake_score(source, target):\n",
    "    return 1.\n",
    "\n",
    "def fake_sample(source):\n",
    "    return [[2,67,33,778,323,68], [545,5347,432,21,53,68,47,3689,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_transformer = MTSampleStreamTransformer(fake_sample, fake_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'sentence_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4ced22a8bc15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfake_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m55\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m324\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m43\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtest_transformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfake_source\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-765caa68a4e2>\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, source_data)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_pair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: global name 'sentence_pair' is not defined"
     ]
    }
   ],
   "source": [
    "fake_source = [1,55,75,324,43,0]\n",
    "\n",
    "test_transformer(fake_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MTSampleStreamGenerator(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_files: list[str] - the files containing the source seqs we'll use to generate samples\n",
    "    sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "    score_func: function\n",
    "    \n",
    "    # source dict only or source + target dict?\n",
    "    dictionary: dict mapping word-->id -- TODO: switch to parameterized choice\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    provides_sources = ('sources','targets', 'scores')\n",
    "    example_iteration_scheme = None\n",
    "\n",
    "    def __init__(self, files, dictionary, all_idxs_and_probs, bos_token=None, eos_token=None,\n",
    "                 unk_token='<UNK>', level='word', preprocess=None):\n",
    "        self.files = files\n",
    "        self.dictionary = dictionary\n",
    "        # use all idxs to choose random substitutions and insertions\n",
    "        # WORKING: switch to parameterized choice using word freq\n",
    "        # WORKING pass in two lists -- words and probs (use the p= kwarg from np.random.choice)\n",
    "        # self.all_idxs = dictionary.values()\n",
    "        self.all_idxs, self.all_probs = zip(*all_idxs_and_probs.items())\n",
    "        if bos_token is not None and bos_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.bos_token = bos_token\n",
    "        if eos_token is not None and eos_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.eos_token = eos_token\n",
    "        if unk_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.unk_token = unk_token\n",
    "        if level not in ('word', 'character'):\n",
    "            raise ValueError\n",
    "        self.level = level\n",
    "        self.preprocess = preprocess\n",
    "        super(RandomTargetGenerator, self).__init__()\n",
    "\n",
    "    def open(self):\n",
    "        return chain(*[iter_(open(f)) for f in self.files])\n",
    "\n",
    "    def get_data(self, state=None, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        sentence = next(state)\n",
    "        if self.preprocess is not None:\n",
    "            sentence = self.preprocess(sentence)\n",
    "        pe_sequence = [self.dictionary[self.bos_token]] if self.bos_token else []\n",
    "        if self.level == 'word':\n",
    "            pe_sequence.extend(self.dictionary.get(word,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for word in sentence.split())\n",
    "        else:\n",
    "            pe_sequence.extend(self.dictionary.get(char,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for char in sentence.strip())\n",
    "        if self.eos_token:\n",
    "            pe_sequence.append(self.dictionary[self.eos_token])\n",
    "\n",
    "        # now use the pe sequence to generate artificial target_sequence and tag_sequence\n",
    "        target_sequence, tag_sequence = self._generate_artificial_target_and_tag_seqs(pe_sequence)\n",
    "\n",
    "        return (target_sequence, tag_sequence)\n",
    "\n",
    "    # TODO: currently only supports subs, not inserts\n",
    "    # TODO: tag identities are hard-coded as 0=BAD 1=OK\n",
    "    def _generate_artificial_target_and_tag_seqs(self, pe_sequence):\n",
    "        target_sequence = copy.copy(pe_sequence)\n",
    "        tag_sequence = [1] * len(pe_sequence)\n",
    "\n",
    "        # TODO: make num_subs distribution configurable\n",
    "        num_subs_to_make = numpy.random.randint(0, (int(numpy.ceil(len(pe_sequence / 1.3)))))\n",
    "        sub_idxs = numpy.random.choice(len(pe_sequence), size=num_subs_to_make)\n",
    "#         print('len orig seq: {}'.format(len(pe_sequence)))\n",
    "#         print('len orig seq / 2: {}'.format(len(pe_sequence) //2))\n",
    "#         print('n_subs: {}'.format(num_subs_to_make))\n",
    "#         print(sub_idxs)\n",
    "        for idx in sub_idxs:\n",
    "            new_word = numpy.random.choice(self.all_idxs, p=self.all_probs)\n",
    "            target_sequence[idx] = new_word\n",
    "            tag_sequence[idx] = 0\n",
    "\n",
    "        return target_sequence, tag_sequence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
