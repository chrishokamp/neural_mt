{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import codecs\n",
    "import tempfile\n",
    "import cPickle\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from picklable_itertools import iter_, chain\n",
    "from fuel.datasets import Dataset\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import numpy\n",
    "import pickle\n",
    "from subprocess import Popen, PIPE\n",
    "import codecs\n",
    "\n",
    "from blocks.algorithms import (GradientDescent, StepClipping,\n",
    "                               CompositeRule, Adam, AdaDelta)\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_noise, apply_dropout\n",
    "from blocks.initialization import IsotropicGaussian, Orthogonal, Constant\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.select import Selector\n",
    "from blocks.search import BeamSearch\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "\n",
    "from machine_translation.checkpoint import CheckpointNMT, LoadNMT\n",
    "from machine_translation.model import BidirectionalEncoder, Decoder\n",
    "from machine_translation.sampling import BleuValidator, Sampler, SamplingBase\n",
    "from machine_translation.stream import (get_tr_stream, get_dev_stream,\n",
    "                                        _ensure_special_tokens)\n",
    "\n",
    "\n",
    "from nnqe.dataset.preprocess import whitespace_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a fuel stream which subclasses text file to create a stream which provides three sources: \n",
    "# (source, [samples], and BLEU scores)\n",
    "\n",
    "# going forward, this may not be the fastest way because the sampling and BLEU score computation can be time consuming\n",
    "# we should look at Fuel's read-ahead and cacheing capacity\n",
    "\n",
    "# TODO: stateful transformer which takes a stream and adds the sources ('samples', 'scores')\n",
    "# class Mapping(Transformer)\n",
    "# the mapping should be a callable which gets samples, then computes the sentence-level BLEU\n",
    "# score for each sample with respect to the reference\n",
    "\n",
    "# use this script to get sentence-level scores(?)\n",
    "# https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v13a.pl\n",
    "\n",
    "# parameters\n",
    "# sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "# score_func: function\n",
    "\n",
    "# TODO: how do they create simple test streams in the fuel tests?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MTSampleStreamTransformer:\n",
    "    \"\"\"\n",
    "    Stateful transformer which takes a stream of (source, target) and adds the sources ('samples', 'scores')\n",
    "    \n",
    "    Samples are generated by calling the sample func with the source as argument\n",
    "    \n",
    "    Scores are generated by comparing each generated sample to the reference\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "    score_func: function\n",
    "\n",
    "    At call time, we expect a stream providing (sources,) -- i.e. something like a TextFile object\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, sample_func, score_func, num_samples=1):\n",
    "        self.sample_func = sample_func\n",
    "        self.score_func = score_func\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \n",
    "        source = data[0]\n",
    "        reference = data[1]\n",
    "        \n",
    "        # each sample may be of different length\n",
    "        samples = self.sample_func(numpy.array(source), self.num_samples)\n",
    "        scores = self._compute_scores(samples, reference)\n",
    "    \n",
    "        return (samples, scores)\n",
    "\n",
    "    # Note that sentence-level BLEU can be computed directly over the indexes (not the strings),\n",
    "    # So we don't need to map back to a string representation\n",
    "    def _compute_scores(self, samples, reference):\n",
    "        \"\"\"Call the scoring function to compare each sample to the reference\"\"\"\n",
    "        \n",
    "        return numpy.ones(len(samples), dtype='float32')\n",
    "\n",
    "\n",
    "# def _get_true_length(seq, vocab):\n",
    "#     try:\n",
    "#         return seq.tolist().index(vocab['</S>']) + 1\n",
    "#     except ValueError:\n",
    "#         return len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fake_score(source, target):\n",
    "    return 1.\n",
    "\n",
    "def fake_sample(source, num_samples=1):\n",
    "    return numpy.vstack(list(itertools.repeat([2,67,33,778,323,68], num_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_transformer = MTSampleStreamTransformer(fake_sample, fake_score, num_samples=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68],\n",
       "        [  2,  67,  33, 778, 323,  68]]),\n",
       " array([ 1.,  1.,  1.,  1.,  1.], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_data = ([[1,55,75,324,43,0], [6546,24,4123,73,85,13]])\n",
    "\n",
    "test_transformer(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating theano variables\n",
      "INFO:root:Building sampling model\n",
      "INFO:root:Creating Sampling Model...\n",
      "INFO:root:Loading parameters from model: /home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/backward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/back_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/back_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/back_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/back_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/forward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/fwd_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/fwd_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/fwd_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/fwd_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000, 300)   : /bidirectionalencoder/embeddings.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /decoder/sequencegenerator/att_trans/decoder.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/att_trans/attention/preprocess.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/att_trans/attention/preprocess.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/attention/state_trans/transform_states.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1)       : /decoder/sequencegenerator/att_trans/attention/energy_comp/linear.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 1600)   : /decoder/sequencegenerator/att_trans/distribute/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/readout/merge/transform_states.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000, 300)   : /decoder/sequencegenerator/readout/lookupfeedbackwmt15/lookuptable.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /decoder/sequencegenerator/readout/merge/transform_feedback.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/readout/merge/transform_weighted_averages.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/maxout_bias.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (400, 300)     : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax0.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 20000)   : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000,)       : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /decoder/sequencegenerator/fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /decoder/sequencegenerator/fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/decoder.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/att_trans/distribute/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /decoder/sequencegenerator/fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Number of parameters loaded for computation graph: 37\n"
     ]
    }
   ],
   "source": [
    "# create the graph which can sample from our model \n",
    "# Note that we must sample instead of getting the 1-best, because we need the randomness to make the expected\n",
    "# BLEU score make sense\n",
    "# TODO: check on the parameterization of the softmax emitter sampler in blocks -- is softmax temperature implemented?\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "    'saved_parameters': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz',\n",
    "    'src_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.en.pkl',\n",
    "    'trg_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.de.pkl',\n",
    "    'src_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.en.tok.shuf',\n",
    "    'trg_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.de.tok.shuf',\n",
    "    'unk_id':1\n",
    "}\n",
    "\n",
    "def get_sampling_model_and_input(exp_config):\n",
    "    # Create Theano variables\n",
    "    encoder = BidirectionalEncoder(\n",
    "        exp_config['src_vocab_size'], exp_config['enc_embed'], exp_config['enc_nhids'])\n",
    "\n",
    "    decoder = Decoder(\n",
    "        exp_config['trg_vocab_size'], exp_config['dec_embed'], exp_config['dec_nhids'],\n",
    "        exp_config['enc_nhids'] * 2)\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    sampling_input = tensor.lmatrix('source')\n",
    "\n",
    "    # Get beam search\n",
    "    logger.info(\"Building sampling model\")\n",
    "    sampling_representation = encoder.apply(\n",
    "        sampling_input, tensor.ones(sampling_input.shape))\n",
    "    generated = decoder.generate(sampling_input, sampling_representation)\n",
    "\n",
    "#     _, samples = VariableFilter(\n",
    "#         bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "#                  ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "#     beam_search = BeamSearch(samples=samples)\n",
    "\n",
    "    # build the model that will let us get a theano function from the sampling graph\n",
    "    logger.info(\"Creating Sampling Model...\")\n",
    "    sampling_model = Model(generated)\n",
    "\n",
    "    # Set the parameters from a trained models\n",
    "    logger.info(\"Loading parameters from model: {}\".format(exp_config['saved_parameters']))\n",
    "    # load the parameter values from an .npz file\n",
    "    param_values = LoadNMT.load_parameter_values(exp_config['saved_parameters'])\n",
    "    LoadNMT.set_model_parameters(sampling_model, param_values)\n",
    "    \n",
    "    return sampling_model, sampling_input\n",
    "\n",
    "test_model, theano_sampling_input = get_sampling_model_and_input(exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test that we can pull samples from the model\n",
    "test_sampling_func = test_model.get_theano_function()\n",
    "trg_vocab = cPickle.load(open(exp_config['trg_vocab']))\n",
    "trg_vocab_size = exp_config['trg_vocab_size'] - 1\n",
    "\n",
    "trg_vocab = _ensure_special_tokens(trg_vocab, bos_idx=0,\n",
    "                                   eos_idx=trg_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "\n",
    "\n",
    "# close over the sampling func and the trg_vocab to standardize the interface\n",
    "# TODO: actually this should be a callable class with params (sampling_func, trg_vocab)\n",
    "# TODO: we may be able to make this function faster by passing multiple sources for sampling at the same damn time\n",
    "def sampling_func(source_seq, target_vocab, num_samples=1):\n",
    "    \n",
    "    def _get_true_length(seq, vocab):\n",
    "        try:\n",
    "            return seq.tolist().index(vocab['</S>']) + 1\n",
    "        except ValueError:\n",
    "            return len(seq)\n",
    "    \n",
    "    print('sample func')\n",
    "    print(source_seq)\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        # outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "        #                 list(next_glimpses.values()) + [next_costs])\n",
    "        _1, outputs, _2, _3, costs = test_sampling_func(source_seq[None, :])\n",
    "        # if we are generating a single sample, the length of the output will be len(source_seq)*2\n",
    "        # see decoder.generate\n",
    "        # the output is a [seq_len, 1] array\n",
    "        outputs = outputs.reshape(outputs.shape[0])\n",
    "        outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        \n",
    "        samples.append(outputs)\n",
    "    \n",
    "#     samples = numpy.vstack(samples)\n",
    "    return samples\n",
    "    \n",
    "    # inp = input_[i, :input_length]\n",
    "#        for _ in range(num_samples):\n",
    "#             for source_seq in a_few_instances[:5]:\n",
    "#                 print(' '.join(src_ivocab[idx] for idx in source_seq))\n",
    "#                 _1, outputs, _2, _3, costs = test_sampling_func(source_seq[None, :])\n",
    "#                 # the output is a [seq_len, 1] array\n",
    "#                 outputs = outputs.reshape(outputs.shape[0])\n",
    "#                 outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "#                 print(' '.join(trg_ivocab[idx] for idx in outputs))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make TextFile streams for source data and references, then merge them together\n",
    "\n",
    "def get_textfile_stream(source_file=None, src_vocab=None, src_vocab_size=30000,\n",
    "                      unk_id=1):\n",
    "    \"\"\"Create a TextFile dataset, and return a stream\"\"\"\n",
    "    \n",
    "    source_stream = None\n",
    "   \n",
    "    src_vocab = _ensure_special_tokens(\n",
    "        src_vocab if isinstance(src_vocab, dict) else\n",
    "        cPickle.load(open(src_vocab)),\n",
    "        bos_idx=0, eos_idx=src_vocab_size - 1, unk_idx=unk_id)\n",
    "    source_dataset = TextFile([source_file], src_vocab, bos_token=None)\n",
    "    source_stream = DataStream(source_dataset)\n",
    "    return source_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_stream = get_textfile_stream(source_file=exp_config['src_data'], src_vocab=exp_config['src_vocab'],\n",
    "                                         src_vocab_size=exp_config['src_vocab_size'])\n",
    "# test_source_stream.sources = ('sources',)\n",
    "\n",
    "trg_stream = get_textfile_stream(source_file=exp_config['trg_data'], src_vocab=exp_config['trg_vocab'],\n",
    "                                         src_vocab_size=exp_config['trg_vocab_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge them to get a source, target pair\n",
    "training_stream = Merge([src_stream,\n",
    "                         trg_stream],\n",
    "                         ('source', 'target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(('samples', 'scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampling_transformer = MTSampleStreamTransformer(sampling_func, fake_score)\n",
    "\n",
    "training_stream = Mapping(training_stream, sampling_transformer, add_sources=('samples', 'scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample func\n",
      "[   17   130     5    28   268   150     7     6   769    11     2   144\n",
      "    71    16     2  1063   282     3 19999]\n",
      "([17, 130, 5, 28, 268, 150, 7, 6, 769, 11, 2, 144, 71, 16, 2, 1063, 282, 3, 19999], [19, 90, 5, 45, 793, 55, 54, 593, 11, 161, 47, 4, 1522, 2, 19999], [array([   19,    90,    20,    45,   382,    55,     9,     4,  1369,\n",
      "           5,     4,  1369,     2, 19999])], array([ 1.], dtype=float32))\n",
      "sample func\n",
      "[    4    10   620    16     6   607   172     7   149    27    43     2\n",
      "   214    12  8915    95   142     3 19999]\n",
      "([4, 10, 620, 16, 6, 607, 172, 7, 149, 27, 43, 2, 214, 12, 8915, 95, 142, 3, 19999], [3, 10, 9, 11, 137, 182, 252, 375, 6, 37, 13, 223, 7, 101, 177, 312, 23, 6634, 2, 19999], [array([    3,   542,   182,     9,    14,   137,     9,    17,   137,\n",
      "           6,    17,    13,   223,    37,     7,   101,   177, 18488,\n",
      "         108,     2, 19999])], array([ 1.], dtype=float32))\n",
      "sample func\n",
      "[    4    58    31    78     2    62  3445    13    19    42   232   722\n",
      "    16     6    38    12   767   520     3 19999]\n",
      "([4, 58, 31, 78, 2, 62, 3445, 13, 19, 42, 232, 722, 16, 6, 38, 12, 767, 520, 3, 19999], [3, 67, 25, 8, 202, 9, 4, 5513, 444, 14, 34, 8, 3037, 357, 2, 19999], [array([    3,    67,    25,     6,    17,    32,   202,   165,     6,\n",
      "         670,     8,    24,   202,     9,     4,  5832,    17,  1607,\n",
      "          34,  6602,   213,     2, 19999])], array([ 1.], dtype=float32))\n",
      "sample func\n",
      "[    4   143   339     5    51    13   598     8    26    89  1237    81\n",
      "   142     3 19999]\n",
      "([4, 143, 339, 5, 51, 13, 598, 8, 26, 89, 1237, 81, 142, 3, 19999], [3, 2204, 5, 541, 99, 6, 5, 198, 110, 13, 2083, 1514, 2, 19999], [array([    3,  2204,     5,  1781,     8,     4,   464,  1514,    64,\n",
      "         336,     9,     4, 10529,     2, 19999])], array([ 1.], dtype=float32))\n",
      "sample func\n",
      "[    4    10   727   233     2    77    81     2   264    38     3 19999]\n",
      "([4, 10, 727, 233, 2, 77, 81, 2, 264, 38, 3, 19999], [3, 10, 289, 23, 5, 14, 110, 11, 343, 34, 21, 13, 105, 2, 19999], [array([    3,    10,    21,     4,   105,    47,    14,    34,     2, 19999])], array([ 1.], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "test_iter = training_stream.get_epoch_iterator()\n",
    "for _ in range(k):\n",
    "    print(test_iter.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now transform the training stream into a stream providing (sources, samples, scores)\n",
    "# sources is tensor.lmatrix\n",
    "# targets is tensor.tensor3 - dtype int64 (N target samples for each source)\n",
    "# scores is tensor.matrix - dtype float32 -- each row contains the N scores for the N samples\n",
    "# generated by the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_source_stream.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_source_data = list(test_source_stream.get_epoch_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(test_source_data)\n",
    "a_few_instances = [numpy.array(i[0]) for i in test_source_data[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def _get_true_length(self, seq, vocab):\n",
    "#     try:\n",
    "#         return seq.tolist().index(vocab['</S>']) + 1\n",
    "#     except ValueError:\n",
    "#         return len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load src and target vocabs to validate samples\n",
    "src_vocab = cPickle.load(open(exp_config['src_vocab']))\n",
    "src_vocab_size = exp_config['src_vocab_size'] - 1\n",
    "trg_vocab_size = exp_config['trg_vocab_size'] - 1\n",
    "\n",
    "\n",
    "src_vocab = _ensure_special_tokens(src_vocab, bos_idx=0,\n",
    "                                   eos_idx=src_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "\n",
    "\n",
    "src_ivocab = {v:k for k,v in src_vocab.items()}\n",
    "trg_ivocab = {v:k for k,v in trg_vocab.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# inp = input_[i, :input_length]\n",
    "\n",
    "def _get_true_length(seq, vocab):\n",
    "    try:\n",
    "        return seq.tolist().index(vocab['</S>']) + 1\n",
    "    except ValueError:\n",
    "        return len(seq)\n",
    "\n",
    "# outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "#                 list(next_glimpses.values()) + [next_costs])\n",
    "num_samples = 5\n",
    "\n",
    "for _ in range(num_samples):\n",
    "    for source_seq in a_few_instances[:5]:\n",
    "        print(' '.join(src_ivocab[idx] for idx in source_seq))\n",
    "        _1, outputs, _2, _3, costs = test_sampling_func(source_seq[None, :])\n",
    "        # the output is a [seq_len, 1] array\n",
    "        outputs = outputs.reshape(outputs.shape[0])\n",
    "        outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        print(' '.join(trg_ivocab[idx] for idx in outputs))\n",
    "\n",
    "# TODO: can we get multiple outputs in one shot?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calling the external scoring script\n",
    "    \n",
    "    self.multibleu_cmd = ['perl', self.config['bleu_script'],\n",
    "                              self.config['val_set_grndtruth'], '<']\n",
    "    \n",
    "    mb_subprocess = Popen(self.multibleu_cmd, stdin=PIPE, stdout=PIPE)\n",
    "    \n",
    "      mb_subprocess.stdin.flush()\n",
    "\n",
    "    \n",
    "     # Write to subprocess and file if it exists\n",
    "    print(trans_out, file=mb_subprocess.stdin)\n",
    "            \n",
    "            \n",
    "        # send end of file, read output.\n",
    "        mb_subprocess.stdin.close()\n",
    "        stdout = mb_subprocess.stdout.readline()\n",
    "        logger.info(stdout)\n",
    "        out_parse = re.match(r'BLEU = [-.0-9]+', stdout)\n",
    "        logger.info(\"Validation Took: {} minutes\".format(\n",
    "            float(time.time() - val_start_time) / 60.))\n",
    "        assert out_parse is not None\n",
    "\n",
    "        # extract the score\n",
    "        bleu_score = float(out_parse.group()[6:])\n",
    "        self.val_bleu_curve.append(bleu_score)\n",
    "        logger.info(bleu_score)\n",
    "        mb_subprocess.terminate()\n",
    "\n",
    "\n",
    "        return bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model related -----------------------------------------------------------\n",
    "# Sequences longer than this will be discarded\n",
    "'seq_len': 40\n",
    "# Number of hidden units in encoder/decoder GRU\n",
    "'enc_nhids': &REC_SIZE 800\n",
    "'dec_nhids': 800\n",
    "\n",
    "# Dimension of the word embedding matrix in encoder/decoder\n",
    "'enc_embed': &EMBED_SIZE 300\n",
    "'dec_embed': 300\n",
    "\n",
    "# Optimization related ----------------------------------------------------\n",
    "# Batch size\n",
    "'batch_size': &BATCH_SIZE 15 \n",
    "\n",
    "# This many batches will be read ahead and sorted\n",
    "'sort_k_batches': 15 \n",
    "\n",
    "# Optimization step rule\n",
    "'step_rule': &STEP_RULE 'Adam'\n",
    "\n",
    "# Gradient clipping threshold\n",
    "'step_clipping': 1.\n",
    "\n",
    "# Std of weight initialization\n",
    "'weight_scale': 0.01\n",
    "\n",
    "# Regularization related --------------------------------------------------\n",
    "\n",
    "# Weight noise flag for feed forward layers\n",
    "'weight_noise_ff': &FF_NOISE False\n",
    "\n",
    "# Weight noise flag for recurrent layers\n",
    "'weight_noise_rec': False\n",
    "\n",
    "# Dropout ratio, applied only after readout maxout\n",
    "'dropout': &DROPOUT 0.3\n",
    "\n",
    "# Source and target vocabulary sizes, should include bos, eos, unk tokens\n",
    "'src_vocab_size': &SRC_VOCAB_SIZE 20000\n",
    "'trg_vocab_size': &TGT_VOCAB_SIZE 20000\n",
    "\n",
    "# Special tokens and indexes\n",
    "'unk_id': 1\n",
    "'bos_token': '<S>'\n",
    "'eos_token': '</S>'\n",
    "'unk_token': '<UNK>'\n",
    "\n",
    "# Root directory for dataset\n",
    "'datadir': &DATADIR /media/1tb_drive/multilingual-multimodal/flickr30k/train/processed\n",
    "\n",
    "# the name of the directory where the model will be saved and checkpointed\n",
    "#'model_save_directory': &SAVEDIR !format_str ['unbabel_data_dropout{}_ff_noise{}_search_model_en2es_vocab{}_emb{}_rec{}_batch{}', *DROPOUT, *FF_NOISE, *SRC_VOCAB_SIZE, *EMBED_SIZE, *REC_SIZE, *BATCH_SIZE]\n",
    "'model_save_directory': &SAVEDIR !format_str ['BERTHA-TEST_{}_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15', *STEP_RULE]\n",
    "\n",
    "# Where to save model, this corresponds to 'prefix' in groundhog\n",
    "'saveto': &OUTPUT_DIR !path_join [*DATADIR, *SAVEDIR]\n",
    "\n",
    "# Module name of the stream that will be used\n",
    "# note this requires the stream to be implemented as a module -- there may be a better way\n",
    "'stream': 'stream'\n",
    "\n",
    "# Source and target vocabularies\n",
    "'src_vocab': !path_join [*DATADIR, 'vocab.en-de.en.pkl']\n",
    "'trg_vocab': !path_join [*DATADIR, 'vocab.en-de.de.pkl']\n",
    "\n",
    "# Source and target datasets\n",
    "'src_data': !path_join [*DATADIR, 'train.en.tok.shuf']\n",
    "'trg_data': !path_join [*DATADIR, 'train.de.tok.shuf']\n",
    "\n",
    "\n",
    "# Early stopping based on BLEU score on dev set ------------------------------------\n",
    "\n",
    "# Normalize cost according to sequence length after beam-search\n",
    "'normalized_bleu': True\n",
    "\n",
    "# Bleu script that will be used (moses multi-perl in this case)\n",
    "'bleu_script': !path_join [*DATADIR, 'multi-bleu.perl']\n",
    "\n",
    "# Validation set source file\n",
    "'val_set': !path_join [*DATADIR, 'dev.en.tok']\n",
    "\n",
    "# Validation set gold file\n",
    "'val_set_grndtruth': !path_join [*DATADIR, 'dev.de.tok']\n",
    "\n",
    "# Print validation output to file\n",
    "'output_val_set': True\n",
    "\n",
    "# Validation output file\n",
    "'val_set_out': !path_join [*OUTPUT_DIR, 'validation_out.txt']\n",
    "\n",
    "# Beam-size\n",
    "'beam_size': 20 \n",
    "\n",
    "# Timing/monitoring related -----------------------------------------------\n",
    "\n",
    "# Maximum number of updates\n",
    "'finish_after': 1000000\n",
    "\n",
    "# Reload model from files if exist\n",
    "'reload': True\n",
    "\n",
    "# Save model after this many updates\n",
    "'save_freq': 5000\n",
    "\n",
    "# Show samples from model after this many updates\n",
    "'sampling_freq': 5000\n",
    "\n",
    "# Show this many samples at each sampling\n",
    "'hook_samples': 5\n",
    "\n",
    "# Validate bleu after this many updates\n",
    "'bleu_val_freq': 1000 \n",
    "\n",
    "# Start bleu validation after this many updates\n",
    "'val_burn_in': 5000\n",
    "\n",
    "# Using trained models for prediction ------------\n",
    "\n",
    "# The location of the saved parameters of a trained model as .npz\n",
    "'saved_parameters': ~\n",
    "\n",
    "# The location of a test set in the source language\n",
    "'test_set': ~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MTSampleStreamGenerator(Dataset):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source_files: list[str] - the files containing the source seqs we'll use to generate samples\n",
    "    sample_func: function(num_samples=1) which takes source seq and outputs <num_samples> samples\n",
    "    score_func: function\n",
    "    \n",
    "    # source dict only or source + target dict?\n",
    "    dictionary: dict mapping word-->id -- TODO: switch to parameterized choice\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    provides_sources = ('sources','targets', 'scores')\n",
    "    example_iteration_scheme = None\n",
    "\n",
    "    def __init__(self, files, dictionary, all_idxs_and_probs, bos_token=None, eos_token=None,\n",
    "                 unk_token='<UNK>', level='word', preprocess=None):\n",
    "        self.files = files\n",
    "        self.dictionary = dictionary\n",
    "        # use all idxs to choose random substitutions and insertions\n",
    "        # WORKING: switch to parameterized choice using word freq\n",
    "        # WORKING pass in two lists -- words and probs (use the p= kwarg from np.random.choice)\n",
    "        # self.all_idxs = dictionary.values()\n",
    "        self.all_idxs, self.all_probs = zip(*all_idxs_and_probs.items())\n",
    "        if bos_token is not None and bos_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.bos_token = bos_token\n",
    "        if eos_token is not None and eos_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.eos_token = eos_token\n",
    "        if unk_token not in dictionary:\n",
    "            raise ValueError\n",
    "        self.unk_token = unk_token\n",
    "        if level not in ('word', 'character'):\n",
    "            raise ValueError\n",
    "        self.level = level\n",
    "        self.preprocess = preprocess\n",
    "        super(RandomTargetGenerator, self).__init__()\n",
    "\n",
    "    def open(self):\n",
    "        return chain(*[iter_(open(f)) for f in self.files])\n",
    "\n",
    "    def get_data(self, state=None, request=None):\n",
    "        if request is not None:\n",
    "            raise ValueError\n",
    "        sentence = next(state)\n",
    "        if self.preprocess is not None:\n",
    "            sentence = self.preprocess(sentence)\n",
    "        pe_sequence = [self.dictionary[self.bos_token]] if self.bos_token else []\n",
    "        if self.level == 'word':\n",
    "            pe_sequence.extend(self.dictionary.get(word,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for word in sentence.split())\n",
    "        else:\n",
    "            pe_sequence.extend(self.dictionary.get(char,\n",
    "                                            self.dictionary[self.unk_token])\n",
    "                        for char in sentence.strip())\n",
    "        if self.eos_token:\n",
    "            pe_sequence.append(self.dictionary[self.eos_token])\n",
    "\n",
    "        # now use the pe sequence to generate artificial target_sequence and tag_sequence\n",
    "        target_sequence, tag_sequence = self._generate_artificial_target_and_tag_seqs(pe_sequence)\n",
    "\n",
    "        return (target_sequence, tag_sequence)\n",
    "\n",
    "    # TODO: currently only supports subs, not inserts\n",
    "    # TODO: tag identities are hard-coded as 0=BAD 1=OK\n",
    "    def _generate_artificial_target_and_tag_seqs(self, pe_sequence):\n",
    "        target_sequence = copy.copy(pe_sequence)\n",
    "        tag_sequence = [1] * len(pe_sequence)\n",
    "\n",
    "        # TODO: make num_subs distribution configurable\n",
    "        num_subs_to_make = numpy.random.randint(0, (int(numpy.ceil(len(pe_sequence / 1.3)))))\n",
    "        sub_idxs = numpy.random.choice(len(pe_sequence), size=num_subs_to_make)\n",
    "#         print('len orig seq: {}'.format(len(pe_sequence)))\n",
    "#         print('len orig seq / 2: {}'.format(len(pe_sequence) //2))\n",
    "#         print('n_subs: {}'.format(num_subs_to_make))\n",
    "#         print(sub_idxs)\n",
    "        for idx in sub_idxs:\n",
    "            new_word = numpy.random.choice(self.all_idxs, p=self.all_probs)\n",
    "            target_sequence[idx] = new_word\n",
    "            tag_sequence[idx] = 0\n",
    "\n",
    "        return target_sequence, tag_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
