{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:test\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import codecs\n",
    "import tempfile\n",
    "import cPickle\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from picklable_itertools import iter_, chain\n",
    "from fuel.datasets import Dataset\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "from fuel.transformers import Transformer\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import numpy\n",
    "import pickle\n",
    "from subprocess import Popen, PIPE\n",
    "import codecs\n",
    "\n",
    "from blocks.algorithms import (GradientDescent, StepClipping,\n",
    "                               CompositeRule, Adam, AdaDelta)\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_noise, apply_dropout\n",
    "from blocks.initialization import IsotropicGaussian, Orthogonal, Constant\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.select import Selector\n",
    "from blocks.search import BeamSearch\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "\n",
    "from machine_translation.checkpoint import CheckpointNMT, LoadNMT\n",
    "from machine_translation.model import BidirectionalEncoder, Decoder\n",
    "from machine_translation.sampling import BleuValidator, Sampler, SamplingBase\n",
    "from machine_translation.stream import (get_tr_stream, get_dev_stream,\n",
    "                                        _ensure_special_tokens, MTSampleStreamTransformer,\n",
    "                                        get_textfile_stream, _too_long, _length, PaddingWithEOS,\n",
    "                                        _oov_to_unk)\n",
    "from machine_translation.evaluation import sentence_level_bleu\n",
    "from machine_translation.models import MinimumRiskSequenceDecoder\n",
    "\n",
    "\n",
    "from nnqe.dataset.preprocess import whitespace_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the training and sampling graphs for minimum risk training\n",
    "# Intialize the MTSampleStreamTransformer with the sampling function\n",
    "\n",
    "# load a model that's already trained, and start tuning it with minimum-risk\n",
    "# mock-up training using the blocks main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     def cost(self, application_call, representation, source_sentence_mask,\n",
    "#              target_samples, target_samples_mask, **kwargs):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: change the way the models are called from the command line to allow more flexibility\n",
    "# TODO: in the way they are trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating theano variables\n",
      "INFO:root:Building sampling model\n",
      "INFO:root:Creating Sampling Model...\n",
      "INFO:root:Loading parameters from model: /home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/backward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/back_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/back_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/back_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/back_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/forward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/fwd_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/fwd_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/fwd_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/fwd_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000, 300)   : /bidirectionalencoder/embeddings.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/decoder/state_initializer/linear_0.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /decoder/sequencegenerator/att_trans/decoder.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/att_trans/attention/preprocess.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/att_trans/attention/preprocess.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/attention/state_trans/transform_states.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1)       : /decoder/sequencegenerator/att_trans/attention/energy_comp/linear.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 1600)   : /decoder/sequencegenerator/att_trans/distribute/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/readout/merge/transform_states.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000, 300)   : /decoder/sequencegenerator/readout/lookupfeedbackwmt15/lookuptable.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /decoder/sequencegenerator/readout/merge/transform_feedback.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/readout/merge/transform_weighted_averages.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/maxout_bias.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (400, 300)     : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax0.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 20000)   : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000,)       : /decoder/sequencegenerator/readout/initializablefeedforwardsequence/softmax1.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /decoder/sequencegenerator/fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /decoder/sequencegenerator/fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /decoder/sequencegenerator/att_trans/decoder.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600, 800)    : /decoder/sequencegenerator/att_trans/distribute/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /decoder/sequencegenerator/fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /decoder/sequencegenerator/fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Number of parameters loaded for computation graph: 37\n"
     ]
    }
   ],
   "source": [
    "# create the graph which can sample from our model \n",
    "# Note that we must sample instead of getting the 1-best or N-best, because we need the randomness to make the expected\n",
    "# BLEU score make sense\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "    'saved_parameters': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz',\n",
    "    'src_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.en.pkl',\n",
    "    'trg_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.de.pkl',\n",
    "    'src_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.en.tok.shuf',\n",
    "    'trg_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.de.tok.shuf',\n",
    "    'unk_id':1,\n",
    "    # Bleu script that will be used (moses multi-perl in this case)\n",
    "    'bleu_script': '/home/chris/projects/neural_mt/test_data/sample_experiment/tiny_demo_dataset/multi-bleu.perl',\n",
    "    # Optimization related ----------------------------------------------------\n",
    "    # Batch size\n",
    "    'batch_size': 2,\n",
    "    # This many batches will be read ahead and sorted\n",
    "    'sort_k_batches': 2,\n",
    "    # Optimization step rule\n",
    "    'step_rule': 'AdaDelta',\n",
    "    # Gradient clipping threshold\n",
    "    'step_clipping': 1.,\n",
    "    # Std of weight initialization\n",
    "    'weight_scale': 0.01,\n",
    "    'seq_len': 40,\n",
    "    'finish_after': 100\n",
    "}\n",
    "\n",
    "def get_sampling_model_and_input(exp_config):\n",
    "    # Create Theano variables\n",
    "    encoder = BidirectionalEncoder(\n",
    "        exp_config['src_vocab_size'], exp_config['enc_embed'], exp_config['enc_nhids'])\n",
    "\n",
    "    decoder = Decoder(\n",
    "        exp_config['trg_vocab_size'], exp_config['dec_embed'], exp_config['dec_nhids'],\n",
    "        exp_config['enc_nhids'] * 2)\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    sampling_input = tensor.lmatrix('source')\n",
    "\n",
    "    # Get beam search\n",
    "    logger.info(\"Building sampling model\")\n",
    "    sampling_representation = encoder.apply(\n",
    "        sampling_input, tensor.ones(sampling_input.shape))\n",
    "    generated = decoder.generate(sampling_input, sampling_representation)\n",
    "\n",
    "#     _, samples = VariableFilter(\n",
    "#         bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "#                  ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "#     beam_search = BeamSearch(samples=samples)\n",
    "\n",
    "    # build the model that will let us get a theano function from the sampling graph\n",
    "    logger.info(\"Creating Sampling Model...\")\n",
    "    sampling_model = Model(generated)\n",
    "\n",
    "    # Set the parameters from a trained models\n",
    "    logger.info(\"Loading parameters from model: {}\".format(exp_config['saved_parameters']))\n",
    "    # load the parameter values from an .npz file\n",
    "    param_values = LoadNMT.load_parameter_values(exp_config['saved_parameters'])\n",
    "    LoadNMT.set_model_parameters(sampling_model, param_values)\n",
    "    \n",
    "    return sampling_model, sampling_input\n",
    "\n",
    "test_model, theano_sampling_input = get_sampling_model_and_input(exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test that we can pull samples from the model\n",
    "test_sampling_func = test_model.get_theano_function()\n",
    "trg_vocab = cPickle.load(open(exp_config['trg_vocab']))\n",
    "trg_vocab_size = exp_config['trg_vocab_size'] - 1\n",
    "\n",
    "trg_vocab = _ensure_special_tokens(trg_vocab, bos_idx=0,\n",
    "                                   eos_idx=trg_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "\n",
    "# close over the sampling func and the trg_vocab to standardize the interface\n",
    "# TODO: actually this should be a callable class with params (sampling_func, trg_vocab)\n",
    "# TODO: we may be able to make this function faster by passing multiple sources for sampling at the same damn time\n",
    "def sampling_func(source_seq, num_samples=1):\n",
    "    print('sampling_func')\n",
    "    print('num_samples: {}'.format(num_samples))\n",
    "    \n",
    "    def _get_true_length(seq, trg_vocab):\n",
    "        try:\n",
    "            return seq.tolist().index(trg_vocab['</S>']) + 1\n",
    "        except ValueError:\n",
    "            return len(seq)\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        # outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "        #                 list(next_glimpses.values()) + [next_costs])\n",
    "        _1, outputs, _2, _3, costs = test_sampling_func(source_seq[None, :])\n",
    "        # if we are generating a single sample, the length of the output will be len(source_seq)*2\n",
    "        # see decoder.generate\n",
    "        # the output is a [seq_len, 1] array\n",
    "        outputs = outputs.reshape(outputs.shape[0])\n",
    "        outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        \n",
    "        samples.append(outputs)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_stream = get_textfile_stream(source_file=exp_config['src_data'], src_vocab=exp_config['src_vocab'],\n",
    "                                         src_vocab_size=exp_config['src_vocab_size'])\n",
    "\n",
    "# test_source_stream.sources = ('sources',)\n",
    "trg_stream = get_textfile_stream(source_file=exp_config['trg_data'], src_vocab=exp_config['trg_vocab'],\n",
    "                                         src_vocab_size=exp_config['trg_vocab_size'])\n",
    "\n",
    "# Merge them to get a source, target pair\n",
    "training_stream = Merge([src_stream,\n",
    "                         trg_stream],\n",
    "                         ('source', 'target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampling_transformer = MTSampleStreamTransformer(sampling_func, fake_score, num_samples=5)\n",
    "sampling_transformer = MTSampleStreamTransformer(sampling_func, sentence_level_bleu, num_samples=5)\n",
    "\n",
    "training_stream = Mapping(training_stream, sampling_transformer, add_sources=('samples', 'scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlattenSamples(Transformer):\n",
    "    \"\"\"Adds padding to variable-length sequences.\n",
    "\n",
    "    When your batches consist of variable-length sequences, use this class\n",
    "    to equalize lengths by adding zero-padding. To distinguish between\n",
    "    data and padding masks can be produced. For each data source that is\n",
    "    masked, a new source will be added. This source will have the name of\n",
    "    the original source with the suffix ``_mask`` (e.g. ``features_mask``).\n",
    "\n",
    "    Elements of incoming batches will be treated as numpy arrays (i.e.\n",
    "    using `numpy.asarray`). If they have more than one dimension,\n",
    "    all dimensions except length, that is the first one, must be equal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "    mask_sources : tuple of strings, optional\n",
    "        The sources for which we need to add a mask. If not provided, a\n",
    "        mask will be created for all data sources\n",
    "    mask_dtype: str, optional\n",
    "        data type of masks. If not provided, floatX from config will\n",
    "        be used.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, mask_sources=None, mask_dtype=None,\n",
    "                 **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        super(FlattenSamples, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "        if mask_sources is None:\n",
    "            mask_sources = self.data_stream.sources\n",
    "        self.mask_sources = mask_sources\n",
    "#         if mask_dtype is None:\n",
    "#             self.mask_dtype = config.floatX\n",
    "#         else:\n",
    "#             self.mask_dtype = mask_dtype\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "#         sources = []\n",
    "#         for source in self.data_stream.sources:\n",
    "#             sources.append(source)\n",
    "#             if source in self.mask_sources:\n",
    "#                 sources.append(source + '_mask')\n",
    "#         return tuple(sources)\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_flattened_samples = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "#             if source not in self.mask_sources:\n",
    "#                 batch_with_masks.append(source_batch)\n",
    "#                 continue\n",
    "            if source == 'samples':\n",
    "                flattened_samples = []\n",
    "                for ins in source_batch:\n",
    "                    for sample in ins:\n",
    "                        flattened_samples.append(sample)\n",
    "                batch_with_flattened_samples.append(flattened_samples)\n",
    "            else:\n",
    "                batch_with_flattened_samples.append(source_batch)\n",
    "                        \n",
    "#             shapes = [numpy.asarray(sample).shape for sample in source_batch]\n",
    "#             lengths = [shape[0] for shape in shapes]\n",
    "#             max_sequence_length = max(lengths)\n",
    "#             rest_shape = shapes[0][1:]\n",
    "#             if not all([shape[1:] == rest_shape for shape in shapes]):\n",
    "#                 raise ValueError(\"All dimensions except length must be equal\")\n",
    "#             dtype = numpy.asarray(source_batch[0]).dtype\n",
    "\n",
    "#             padded_batch = numpy.zeros(\n",
    "#                 (len(source_batch), max_sequence_length) + rest_shape,\n",
    "#                 dtype=dtype)\n",
    "#             for i, sample in enumerate(source_batch):\n",
    "#                 padded_batch[i, :len(sample)] = sample\n",
    "#             batch_with_masks.append(padded_batch)\n",
    "\n",
    "#             mask = numpy.zeros((len(source_batch), max_sequence_length),\n",
    "#                                self.mask_dtype)\n",
    "#             for i, sequence_length in enumerate(lengths):\n",
    "#                 mask[i, :sequence_length] = 1\n",
    "        return tuple(batch_with_flattened_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: now call the main() function with the train and dev streams \n",
    "# TODO: test that the naive cost function works\n",
    "\n",
    " # Filter sequences that are too long\n",
    "# training_stream = Filter(training_stream,\n",
    "#                 predicate=_too_long(seq_len=exp_config['seq_len']))\n",
    "\n",
    "# Replace out of vocabulary tokens with unk token\n",
    "# training_stream = Mapping(training_stream,\n",
    "#                  _oov_to_unk(src_vocab_size=exp_config['src_vocab_size'],\n",
    "#                              trg_vocab_size=exp_config['trg_vocab_size'],\n",
    "#                              unk_id=exp_config['unk_id']))\n",
    "\n",
    "# Build a batched version of stream to read k batches ahead\n",
    "training_stream = Batch(training_stream,\n",
    "               iteration_scheme=ConstantScheme(\n",
    "                   exp_config['batch_size']*exp_config['sort_k_batches']))\n",
    "\n",
    "# Sort all samples in the read-ahead batch\n",
    "training_stream = Mapping(training_stream, SortMapping(_length))\n",
    "\n",
    "# Convert it into a stream again\n",
    "training_stream = Unpack(training_stream)\n",
    "\n",
    "# Construct batches from the stream with specified batch size\n",
    "training_stream = Batch(\n",
    "    training_stream, iteration_scheme=ConstantScheme(exp_config['batch_size']))\n",
    "\n",
    "# Pad sequences that are short\n",
    "# TODO: padding needs to be done over the flattened samples, or the Padding implementation needs to be\n",
    "# modified --\n",
    "# IDEA: add a transformer which flattens the target samples before we add the mask\n",
    "flat_sample_stream = FlattenSamples(training_stream)\n",
    "\n",
    "# TODO: some sources can be excluded from the padding Op\n",
    "masked_stream = PaddingWithEOS(\n",
    "    flat_sample_stream, [exp_config['src_vocab_size'] - 1, exp_config['trg_vocab_size'] - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(config, tr_stream, dev_stream, use_bokeh=False):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "    \n",
    "#     target_samples = tensor.tensor3('samples').astype('int64')\n",
    "#     target_samples_mask = tensor.tensor3('target_samples_mask').astype('int64')\n",
    "    samples = tensor.lmatrix('samples')\n",
    "    samples_mask = tensor.matrix('samples_mask')\n",
    "    \n",
    "    # scores is (batch, samples)\n",
    "    scores = tensor.matrix('scores')\n",
    "    # We don't need a scores mask because there should be the same number of scores for each instance\n",
    "    # num samples is a hyperparameter of the model\n",
    "    \n",
    "#     samples_mask = tensor.matrix('s_mask')\n",
    "    \n",
    "    sampling_input = tensor.lmatrix('input')\n",
    "\n",
    "    # Construct model\n",
    "    logger.info('Building RNN encoder-decoder')\n",
    "    encoder = BidirectionalEncoder(\n",
    "        config['src_vocab_size'], config['enc_embed'], config['enc_nhids'])\n",
    "#     decoder = MinimumRiskSequenceDecoder(\n",
    "#         config['trg_vocab_size'], config['dec_embed'], config['dec_nhids'],\n",
    "#         config['enc_nhids'] * 2)\n",
    "    decoder = Decoder(\n",
    "        config['trg_vocab_size'], config['dec_embed'], config['dec_nhids'],\n",
    "        config['enc_nhids'] * 2)\n",
    "    \n",
    "    # the name is important to make sure pre-trained params get loaded correctly\n",
    "#     decoder.name = 'decoder'\n",
    "    \n",
    "    # This is the part that is different for the MinimumRiskSequenceGenerator\n",
    "    # WORKING: do we need to modify the sequence_generator directly??\n",
    "    cost = decoder.expected_cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, samples, samples_mask, scores)\n",
    "#     cost = decoder.cost(\n",
    "#         encoder.apply(source_sentence, source_sentence_mask),\n",
    "#         source_sentence_mask, target_samples, target_samples_mask)\n",
    "\n",
    "    logger.info('Creating computational graph')\n",
    "    cg = ComputationGraph(cost)\n",
    "    \n",
    "    # Initialize model\n",
    "#     logger.info('Initializing model')\n",
    "#     encoder.weights_init = decoder.weights_init = IsotropicGaussian(\n",
    "#         config['weight_scale'])\n",
    "#     encoder.biases_init = decoder.biases_init = Constant(0)\n",
    "#     encoder.push_initialization_config()\n",
    "#     decoder.push_initialization_config()\n",
    "#     encoder.bidir.prototype.weights_init = Orthogonal()\n",
    "#     decoder.transition.weights_init = Orthogonal()\n",
    "#     encoder.initialize()\n",
    "#     decoder.initialize()\n",
    "\n",
    "    # WORKING: set weights for the decoder model\n",
    "    \n",
    "    \n",
    "    # apply dropout for regularization\n",
    "#     if config['dropout'] < 1.0:\n",
    "        # dropout is applied to the output of maxout in ghog\n",
    "        # this is the probability of dropping out, so you probably want to make it <=0.5\n",
    "#         logger.info('Applying dropout')\n",
    "#         dropout_inputs = [x for x in cg.intermediary_variables\n",
    "#                           if x.name == 'maxout_apply_output']\n",
    "#         cg = apply_dropout(cg, dropout_inputs, config['dropout'])\n",
    "\n",
    "    # Apply weight noise for regularization\n",
    "#     if config['weight_noise_ff'] > 0.0:\n",
    "#         logger.info('Applying weight noise to ff layers')\n",
    "#         enc_params = Selector(encoder.lookup).get_parameters().values()\n",
    "#         enc_params += Selector(encoder.fwd_fork).get_parameters().values()\n",
    "#         enc_params += Selector(encoder.back_fork).get_parameters().values()\n",
    "#         dec_params = Selector(\n",
    "#             decoder.sequence_generator.readout).get_parameters().values()\n",
    "#         dec_params += Selector(\n",
    "#             decoder.sequence_generator.fork).get_parameters().values()\n",
    "#         dec_params += Selector(decoder.transition.initial_transformer).get_parameters().values()\n",
    "#         cg = apply_noise(cg, enc_params+dec_params, config['weight_noise_ff'])\n",
    "\n",
    "    # TODO: weight noise for recurrent params isn't currently implemented -- see config['weight_noise_rec']\n",
    "    # Print shapes\n",
    "#     shapes = [param.get_value().shape for param in cg.parameters]\n",
    "#     logger.info(\"Parameter shapes: \")\n",
    "#     for shape, count in Counter(shapes).most_common():\n",
    "#         logger.info('    {:15}: {}'.format(shape, count))\n",
    "#     logger.info(\"Total number of parameters: {}\".format(len(shapes)))\n",
    "\n",
    "    # Print parameter names\n",
    "#     enc_dec_param_dict = merge(Selector(encoder).get_parameters(),\n",
    "#                                Selector(decoder).get_parameters())\n",
    "#     logger.info(\"Parameter names: \")\n",
    "#     for name, value in enc_dec_param_dict.items():\n",
    "#         logger.info('    {:15}: {}'.format(value.get_value().shape, name))\n",
    "#     logger.info(\"Total number of parameters: {}\"\n",
    "#                 .format(len(enc_dec_param_dict)))\n",
    "\n",
    "    # Set up training model\n",
    "    logger.info(\"Building model\")\n",
    "    training_model = Model(cost)\n",
    "    \n",
    "    logger.info(\"Loading parameters from model: {}\".format(config['saved_parameters']))\n",
    "    # load the parameter values from an .npz file\n",
    "    param_values = LoadNMT.load_parameter_values(config['saved_parameters'])\n",
    "    LoadNMT.set_model_parameters(training_model, param_values)\n",
    "\n",
    "    # create the training directory, and copy this config there if directory doesn't exist\n",
    "#     if not os.path.isdir(config['saveto']):\n",
    "#         os.makedirs(config['saveto'])\n",
    "#         shutil.copy(config['config_file'], config['saveto'])\n",
    "\n",
    "    # Set extensions\n",
    "    logger.info(\"Initializing extensions\")\n",
    "    extensions = [\n",
    "        FinishAfter(after_n_batches=config['finish_after']),\n",
    "        TrainingDataMonitoring([cost], after_batch=True),\n",
    "        Printing(after_batch=True),\n",
    "#         CheckpointNMT(config['saveto'],\n",
    "#                       every_n_batches=config['save_freq'])\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Set up beam search and sampling computation graphs if necessary\n",
    "    \n",
    "#     if config['hook_samples'] >= 1 or config['bleu_script'] is not None:\n",
    "#         logger.info(\"Building sampling model\")\n",
    "#         sampling_representation = encoder.apply(\n",
    "#             sampling_input, tensor.ones(sampling_input.shape))\n",
    "#         # TODO: the generated output actually contains several more values, ipdb to see what they are\n",
    "#         generated = decoder.generate(sampling_input, sampling_representation)\n",
    "#         search_model = Model(generated)\n",
    "#         _, samples = VariableFilter(\n",
    "#             bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "#                 ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "\n",
    "    # Add sampling\n",
    "#     if config['hook_samples'] >= 1:\n",
    "#         logger.info(\"Building sampler\")\n",
    "#         extensions.append(\n",
    "#             Sampler(model=search_model, data_stream=tr_stream,\n",
    "#                     hook_samples=config['hook_samples'],\n",
    "#                     every_n_batches=config['sampling_freq'],\n",
    "#                     src_vocab_size=config['src_vocab_size']))\n",
    "\n",
    "    # Add early stopping based on bleu\n",
    "#     if config['bleu_script'] is not None:\n",
    "#         logger.info(\"Building bleu validator\")\n",
    "#         extensions.append(\n",
    "#             BleuValidator(sampling_input, samples=samples, config=config,\n",
    "#                           model=search_model, data_stream=dev_stream,\n",
    "#                           normalize=config['normalized_bleu'],\n",
    "#                           every_n_batches=config['bleu_val_freq']))\n",
    "\n",
    "    # Reload model if necessary\n",
    "#     if config['reload']:\n",
    "#         extensions.append(LoadNMT(config['saveto']))\n",
    "\n",
    "    # Plot cost in bokeh if necessary\n",
    "#     if use_bokeh and BOKEH_AVAILABLE:\n",
    "#         extensions.append(\n",
    "#             Plot(config['model_save_directory'], channels=[['decoder_cost_cost'], ['validation_set_bleu_score']],\n",
    "#                  every_n_batches=10))\n",
    "\n",
    "    # Set up training algorithm\n",
    "    logger.info(\"Initializing training algorithm\")\n",
    "    # if there is dropout or random noise, we need to use the output of the modified graph\n",
    "#     if config['dropout'] < 1.0 or config['weight_noise_ff'] > 0.0:\n",
    "#         algorithm = GradientDescent(\n",
    "#             cost=cg.outputs[0], parameters=cg.parameters,\n",
    "#             step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "#                                      eval(config['step_rule'])()])\n",
    "#         )\n",
    "#     else:\n",
    "#         algorithm = GradientDescent(\n",
    "#             cost=cost, parameters=cg.parameters,\n",
    "#             step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "#                                      eval(config['step_rule'])()])\n",
    "#         )\n",
    "\n",
    "    algorithm = GradientDescent(\n",
    "        cost=cost, parameters=cg.parameters,\n",
    "        step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "                                 eval(config['step_rule'])()],\n",
    "                               ),\n",
    "        on_unused_sources='warn'\n",
    "    )\n",
    "\n",
    "    # enrich the logged information\n",
    "    extensions.append(\n",
    "        Timing(every_n_batches=100)\n",
    "    )\n",
    "\n",
    "    # Initialize main loop\n",
    "    logger.info(\"Initializing main loop\")\n",
    "    main_loop = MainLoop(\n",
    "        model=training_model,\n",
    "        algorithm=algorithm,\n",
    "        data_stream=tr_stream,\n",
    "        extensions=extensions\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Creating theano variables\n",
      "INFO:root:Building RNN encoder-decoder\n",
      "INFO:root:Creating computational graph\n",
      "INFO:root:Building model\n",
      "INFO:root:Loading parameters from model: /home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/backward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/back_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/back_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/back_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/back_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/bidirectionalwmt15/forward.initial_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (1600,)        : /bidirectionalencoder/fwd_fork/fork_gate_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 1600)    : /bidirectionalencoder/fwd_fork/fork_gate_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800,)         : /bidirectionalencoder/fwd_fork/fork_inputs.b\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (300, 800)     : /bidirectionalencoder/fwd_fork/fork_inputs.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (20000, 300)   : /bidirectionalencoder/embeddings.W\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/forward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 1600)    : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_gates\n",
      "INFO:machine_translation.checkpoint: Loaded to CG (800, 800)     : /bidirectionalencoder/bidirectionalwmt15/backward.state_to_state\n",
      "INFO:machine_translation.checkpoint: Number of parameters loaded for computation graph: 15\n",
      "INFO:root:Initializing extensions\n",
      "DEBUG:blocks.monitoring.evaluators:variable to evaluate: decoder_expected_cost_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Using the default  (average over minibatches) aggregation scheme for decoder_expected_cost_cost\n",
      "DEBUG:blocks.monitoring.evaluators:Compiling initialization and readout functions\n",
      "DEBUG:blocks.monitoring.evaluators:Initialization and readout functions compiled\n",
      "INFO:root:Initializing training algorithm\n",
      "INFO:blocks.algorithms:Taking the cost gradient\n",
      "INFO:blocks.algorithms:The cost gradient computation graph is built\n",
      "INFO:root:Initializing main loop\n",
      "INFO:blocks.main_loop:Entered the main loop\n",
      "INFO:blocks.algorithms:Initializing the training algorithm\n",
      "INFO:blocks.algorithms:The training algorithm is initialized\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpqGfaDG nmt src< /tmp/tmpqGfaDG\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpfIt1Ku nmt ref< /tmp/tmpx7ymna\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpfIt1Ku nmt tst< /tmp/tmp903VHA\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpAAEDui -s /tmp/tmpfIt1Ku -t /tmp/tmpxpw2FU -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------\n",
      "BEFORE FIRST EPOCH\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 0\n",
      "\t received_first_batch: False\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 0:\n",
      "\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpERNDvO nmt src< /tmp/tmpERNDvO\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpVq5Fc9 nmt ref< /tmp/tmpkWCZBR\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpVq5Fc9 nmt tst< /tmp/tmpFrcmfs\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmp6fzaWM -s /tmp/tmpVq5Fc9 -t /tmp/tmpW4Qvxy -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpiTJDFk nmt src< /tmp/tmpiTJDFk\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpCgMfgb nmt ref< /tmp/tmpCsTznu\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpCgMfgb nmt tst< /tmp/tmpkKGejP\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmp9TgT1M -s /tmp/tmpCgMfgb -t /tmp/tmpWUP0zR -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpTNmWE4 nmt src< /tmp/tmpTNmWE4\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpeIJW4B nmt ref< /tmp/tmpWHMhj6\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpeIJW4B nmt tst< /tmp/tmpYZ5ZL7\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmp_CC9jA -s /tmp/tmpeIJW4B -t /tmp/tmphZ01r4 -b -d 2\n",
      "WARNING:blocks.algorithms:\n",
      "\n",
      "Blocks tried to match the sources (['source_mask', 'target_mask', 'scores_mask', 'samples_mask', 'source', 'samples', 'scores', 'target']) of the training dataset to the names of the Theano variables (['source_mask', 'source']), but failed to do so. If you want to train on a subset of the sources that your dataset provides, pass the `sources` keyword argument to its constructor. Or pass on_unused_sources='warn' or on_unused_sources='ignore' to the GradientDescent algorithm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 1\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 1:\n",
      "\t decoder_expected_cost_cost: -127.369415283\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpsXPS_o nmt src< /tmp/tmpsXPS_o\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpOLN6Z4 nmt ref< /tmp/tmpvufpBs\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpOLN6Z4 nmt tst< /tmp/tmp4xzevc\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmprmv_1A -s /tmp/tmpOLN6Z4 -t /tmp/tmpk2HLGK -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 2\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 2:\n",
      "\t decoder_expected_cost_cost: -314.909912109\n",
      "\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpMpvEON nmt src< /tmp/tmpMpvEON\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpUKB3H9 nmt ref< /tmp/tmpV2BbdF\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpUKB3H9 nmt tst< /tmp/tmp87U_Nv\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpsiBBjx -s /tmp/tmpUKB3H9 -t /tmp/tmpkkivTD -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpMyClJD nmt src< /tmp/tmpMyClJD\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmph6QcOL nmt ref< /tmp/tmp8fvsat\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmph6QcOL nmt tst< /tmp/tmppEQlGP\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpF3nckY -s /tmp/tmph6QcOL -t /tmp/tmpz339t2 -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpmIKSZW nmt src< /tmp/tmpmIKSZW\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpyFu51f nmt ref< /tmp/tmp4M6mr0\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpyFu51f nmt tst< /tmp/tmpYwHHFK\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpLKZN20 -s /tmp/tmpyFu51f -t /tmp/tmp4ly0ML -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 3\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 3:\n",
      "\t decoder_expected_cost_cost: -470.686065674\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpdNLjR3 nmt src< /tmp/tmpdNLjR3\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpv35vYg nmt ref< /tmp/tmp3_XHrc\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpv35vYg nmt tst< /tmp/tmpTphHIa\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmp49DXHt -s /tmp/tmpv35vYg -t /tmp/tmpo3ESQH -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 4\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 4:\n",
      "\t decoder_expected_cost_cost: -518.368286133\n",
      "\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpQFK3VE nmt src< /tmp/tmpQFK3VE\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpw8r9kF nmt ref< /tmp/tmpO_hkss\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpw8r9kF nmt tst< /tmp/tmpjtxy9m\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpGj8qEq -s /tmp/tmpw8r9kF -t /tmp/tmpQ8wn1a -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpoKktmV nmt src< /tmp/tmpoKktmV\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpTGhJYK nmt ref< /tmp/tmp_AXVKN\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpTGhJYK nmt tst< /tmp/tmpht_8hT\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpYD4le5 -s /tmp/tmpTGhJYK -t /tmp/tmpxl8yBn -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpX5puzS nmt src< /tmp/tmpX5puzS\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpEwUxz9 nmt ref< /tmp/tmpyFeUId\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpEwUxz9 nmt tst< /tmp/tmpuDnYxc\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmphi5vFR -s /tmp/tmpEwUxz9 -t /tmp/tmpgdwdJF -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 5\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 5:\n",
      "\t decoder_expected_cost_cost: -921.005371094\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmpIUmZxu nmt src< /tmp/tmpIUmZxu\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpMF_N8d nmt ref< /tmp/tmpFCW3_W\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpMF_N8d nmt tst< /tmp/tmpoVCzSo\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpP0TCEb -s /tmp/tmpMF_N8d -t /tmp/tmpdVQuv6 -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "-------------------------------------------------------------------------------\n",
      "Training status:\n",
      "\t batch_interrupt_received: False\n",
      "\t epoch_interrupt_received: False\n",
      "\t epoch_started: True\n",
      "\t epochs_done: 0\n",
      "\t iterations_done: 6\n",
      "\t received_first_batch: True\n",
      "\t resumed_from: None\n",
      "\t training_started: True\n",
      "Log records from the iteration 6:\n",
      "\t decoder_expected_cost_cost: -770.002990723\n",
      "\n",
      "sampling_func\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl en /tmp/tmphlOHud nmt src< /tmp/tmphlOHud\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpWKdqWE nmt ref< /tmp/tmpI9onEz\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/wrap-xml-modified.perl de /tmp/tmpWKdqWE nmt tst< /tmp/tmpGEhnGm\n",
      "DEBUG:root:perl /home/chris/projects/neural_mt/scripts/scoring/mteval-v13a.pl -r /tmp/tmpC03hqx -s /tmp/tmpWKdqWE -t /tmp/tmpxpF7IQ -b -d 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "num_samples: 5\n",
      "sampling_func"
     ]
    }
   ],
   "source": [
    "main(exp_config, masked_stream, dev_stream=None, use_bokeh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked_stream.get_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
