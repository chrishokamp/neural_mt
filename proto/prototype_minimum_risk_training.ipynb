{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import codecs\n",
    "import tempfile\n",
    "import cPickle\n",
    "import os\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "from fuel.datasets import H5PYDataset\n",
    "from picklable_itertools import iter_, chain\n",
    "from fuel.datasets import Dataset\n",
    "from fuel.datasets import TextFile\n",
    "from fuel.schemes import ConstantScheme\n",
    "from fuel.streams import DataStream\n",
    "from fuel.transformers import (\n",
    "    Merge, Batch, Filter, Padding, SortMapping, Unpack, Mapping)\n",
    "from fuel.transformers import Transformer\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from collections import Counter\n",
    "from theano import tensor\n",
    "from toolz import merge\n",
    "import numpy\n",
    "import pickle\n",
    "from subprocess import Popen, PIPE\n",
    "import codecs\n",
    "\n",
    "from blocks.algorithms import (GradientDescent, StepClipping,\n",
    "                               CompositeRule, Adam, AdaDelta)\n",
    "from blocks.extensions import FinishAfter, Printing, Timing\n",
    "from blocks.extensions.monitoring import TrainingDataMonitoring\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.graph import ComputationGraph, apply_noise, apply_dropout\n",
    "from blocks.initialization import IsotropicGaussian, Orthogonal, Constant\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.model import Model\n",
    "from blocks.select import Selector\n",
    "from blocks.search import BeamSearch\n",
    "from blocks_extras.extensions.plot import Plot\n",
    "\n",
    "from machine_translation.checkpoint import CheckpointNMT, LoadNMT\n",
    "from machine_translation.model import BidirectionalEncoder, Decoder\n",
    "from machine_translation.sampling import BleuValidator, Sampler, SamplingBase\n",
    "from machine_translation.stream import (get_tr_stream, get_dev_stream,\n",
    "                                        _ensure_special_tokens, MTSampleStreamTransformer,\n",
    "                                        get_textfile_stream, _too_long, _length, PaddingWithEOS,\n",
    "                                        _oov_to_unk)\n",
    "from machine_translation.evaluation import sentence_level_bleu\n",
    "\n",
    "\n",
    "from nnqe.dataset.preprocess import whitespace_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the training and sampling graphs for minimum risk training\n",
    "# Intialize the MTSampleStreamTransformer with the sampling function\n",
    "\n",
    "# load a model that's already trained, and start tuning it with minimum-risk\n",
    "# mock-up training using the blocks main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     def cost(self, application_call, representation, source_sentence_mask,\n",
    "#              target_samples, target_samples_mask, **kwargs):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: change the way the models are called from the command line to allow more flexibility\n",
    "# TODO: in the way they are trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the graph which can sample from our model \n",
    "# Note that we must sample instead of getting the 1-best or N-best, because we need the randomness to make the expected\n",
    "# BLEU score make sense\n",
    "\n",
    "exp_config = {\n",
    "    'src_vocab_size': 20000,\n",
    "    'trg_vocab_size': 20000,\n",
    "    'enc_embed': 300,\n",
    "    'dec_embed': 300,\n",
    "    'enc_nhids': 800,\n",
    "    'dec_nhids': 800,\n",
    "    'saved_parameters': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/best_bleu_model_1455464992_BLEU31.61.npz',\n",
    "    'src_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.en.pkl',\n",
    "    'trg_vocab': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout0'+\\\n",
    "    '.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/vocab.en-de.de.pkl',\n",
    "    'src_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.en.tok.shuf',\n",
    "    'trg_data': '/home/chris/projects/neural_mt/archived_models/BERTHA-TEST_Adam_wmt-multimodal_internal_data_dropout'+\\\n",
    "    '0.3_ff_noiseFalse_search_model_en2es_vocab20000_emb300_rec800_batch15/training_data/train.de.tok.shuf',\n",
    "    'unk_id':1,\n",
    "    # Bleu script that will be used (moses multi-perl in this case)\n",
    "    'bleu_script': '/home/chris/projects/neural_mt/test_data/sample_experiment/tiny_demo_dataset/multi-bleu.perl',\n",
    "    # Optimization related ----------------------------------------------------\n",
    "    # Batch size\n",
    "    'batch_size': 2,\n",
    "    # This many batches will be read ahead and sorted\n",
    "    'sort_k_batches': 1,\n",
    "    # Optimization step rule\n",
    "    'step_rule': 'AdaDelta',\n",
    "    # Gradient clipping threshold\n",
    "    'step_clipping': 1.,\n",
    "    # Std of weight initialization\n",
    "    'weight_scale': 0.01,\n",
    "    'seq_len': 40,\n",
    "    'finish_after': 100\n",
    "}\n",
    "\n",
    "def get_sampling_model_and_input(exp_config):\n",
    "    # Create Theano variables\n",
    "    encoder = BidirectionalEncoder(\n",
    "        exp_config['src_vocab_size'], exp_config['enc_embed'], exp_config['enc_nhids'])\n",
    "\n",
    "    decoder = Decoder(\n",
    "        exp_config['trg_vocab_size'], exp_config['dec_embed'], exp_config['dec_nhids'],\n",
    "        exp_config['enc_nhids'] * 2)\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    sampling_input = tensor.lmatrix('source')\n",
    "\n",
    "    # Get beam search\n",
    "    logger.info(\"Building sampling model\")\n",
    "    sampling_representation = encoder.apply(\n",
    "        sampling_input, tensor.ones(sampling_input.shape))\n",
    "    generated = decoder.generate(sampling_input, sampling_representation)\n",
    "\n",
    "#     _, samples = VariableFilter(\n",
    "#         bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "#                  ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "#     beam_search = BeamSearch(samples=samples)\n",
    "\n",
    "    # build the model that will let us get a theano function from the sampling graph\n",
    "    logger.info(\"Creating Sampling Model...\")\n",
    "    sampling_model = Model(generated)\n",
    "\n",
    "    # Set the parameters from a trained models\n",
    "    logger.info(\"Loading parameters from model: {}\".format(exp_config['saved_parameters']))\n",
    "    # load the parameter values from an .npz file\n",
    "    param_values = LoadNMT.load_parameter_values(exp_config['saved_parameters'])\n",
    "    LoadNMT.set_model_parameters(sampling_model, param_values)\n",
    "    \n",
    "    return sampling_model, sampling_input, encoder, decoder\n",
    "\n",
    "test_model, theano_sampling_input, train_encoder, train_decoder = get_sampling_model_and_input(exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test that we can pull samples from the model\n",
    "test_sampling_func = test_model.get_theano_function()\n",
    "trg_vocab = cPickle.load(open(exp_config['trg_vocab']))\n",
    "trg_vocab_size = exp_config['trg_vocab_size'] - 1\n",
    "src_vocab = cPickle.load(open(exp_config['src_vocab']))\n",
    "src_vocab_size = exp_config['src_vocab_size'] - 1\n",
    "\n",
    "src_vocab = _ensure_special_tokens(src_vocab, bos_idx=0,\n",
    "                                   eos_idx=src_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "trg_vocab = _ensure_special_tokens(trg_vocab, bos_idx=0,\n",
    "                                   eos_idx=trg_vocab_size, unk_idx=exp_config['unk_id'])\n",
    "\n",
    "# close over the sampling func and the trg_vocab to standardize the interface\n",
    "# TODO: actually this should be a callable class with params (sampling_func, trg_vocab)\n",
    "# TODO: we may be able to make this function faster by passing multiple sources for sampling at the same damn time\n",
    "def sampling_func(source_seq, num_samples=1):\n",
    "    print('sampling_func')\n",
    "    print('num_samples: {}'.format(num_samples))\n",
    "    \n",
    "    def _get_true_length(seq, trg_vocab):\n",
    "        try:\n",
    "            return seq.tolist().index(trg_vocab['</S>']) + 1\n",
    "        except ValueError:\n",
    "            return len(seq)\n",
    "    \n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        # outputs of self.sampling_fn = outputs of sequence_generator.generate: next_states + [next_outputs] +\n",
    "        #                 list(next_glimpses.values()) + [next_costs])\n",
    "        _1, outputs, _2, _3, costs = test_sampling_func(source_seq[None, :])\n",
    "        # if we are generating a single sample, the length of the output will be len(source_seq)*2\n",
    "        # see decoder.generate\n",
    "        # the output is a [seq_len, 1] array\n",
    "        outputs = outputs.reshape(outputs.shape[0])\n",
    "        outputs = outputs[:_get_true_length(outputs, trg_vocab)]\n",
    "        \n",
    "        samples.append(outputs)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "src_stream = get_textfile_stream(source_file=exp_config['src_data'], src_vocab=exp_config['src_vocab'],\n",
    "                                         src_vocab_size=exp_config['src_vocab_size'])\n",
    "\n",
    "# test_source_stream.sources = ('sources',)\n",
    "trg_stream = get_textfile_stream(source_file=exp_config['trg_data'], src_vocab=exp_config['trg_vocab'],\n",
    "                                         src_vocab_size=exp_config['trg_vocab_size'])\n",
    "\n",
    "# Merge them to get a source, target pair\n",
    "training_stream = Merge([src_stream,\n",
    "                         trg_stream],\n",
    "                         ('source', 'target'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sampling_transformer = MTSampleStreamTransformer(sampling_func, fake_score, num_samples=5)\n",
    "sampling_transformer = MTSampleStreamTransformer(sampling_func, sentence_level_bleu, num_samples=5)\n",
    "\n",
    "training_stream = Mapping(training_stream, sampling_transformer, add_sources=('samples', 'scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlattenSamples(Transformer):\n",
    "    \"\"\"Adds padding to variable-length sequences.\n",
    "\n",
    "    When your batches consist of variable-length sequences, use this class\n",
    "    to equalize lengths by adding zero-padding. To distinguish between\n",
    "    data and padding masks can be produced. For each data source that is\n",
    "    masked, a new source will be added. This source will have the name of\n",
    "    the original source with the suffix ``_mask`` (e.g. ``features_mask``).\n",
    "\n",
    "    Elements of incoming batches will be treated as numpy arrays (i.e.\n",
    "    using `numpy.asarray`). If they have more than one dimension,\n",
    "    all dimensions except length, that is the first one, must be equal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        super(FlattenSamples, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "#         if mask_dtype is None:\n",
    "#             self.mask_dtype = config.floatX\n",
    "#         else:\n",
    "#             self.mask_dtype = mask_dtype\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "#         sources = []\n",
    "#         for source in self.data_stream.sources:\n",
    "#             sources.append(source)\n",
    "#             if source in self.mask_sources:\n",
    "#                 sources.append(source + '_mask')\n",
    "#         return tuple(sources)\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_flattened_samples = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "#             if source not in self.mask_sources:\n",
    "#                 batch_with_masks.append(source_batch)\n",
    "#                 continue\n",
    "            if source == 'samples':\n",
    "                flattened_samples = []\n",
    "                for ins in source_batch:\n",
    "                    for sample in ins:\n",
    "                        flattened_samples.append(sample)\n",
    "                batch_with_flattened_samples.append(flattened_samples)\n",
    "            else:\n",
    "                batch_with_flattened_samples.append(source_batch)\n",
    "                        \n",
    "        return tuple(batch_with_flattened_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CopySourceNTimes(Transformer):\n",
    "    \"\"\"Duplicate the source N times to match the number of samples\n",
    "    \n",
    "    We need this transformer because the attention model expects one source sequence for each\n",
    "    target sequence, but in the sampling case there are effectively (instances*sample_size) target sequences\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stream : :class:`AbstractDataStream` instance\n",
    "        The data stream to wrap\n",
    "    n_samples : int -- the number of samples that were generated for each source sequence\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, data_stream, n_samples=5, **kwargs):\n",
    "        if data_stream.produces_examples:\n",
    "            raise ValueError('the wrapped data stream must produce batches of '\n",
    "                             'examples, not examples')\n",
    "        self.n_samples = n_samples    \n",
    "            \n",
    "        super(CopySourceNTimes, self).__init__(\n",
    "            data_stream, produces_examples=False, **kwargs)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def sources(self):\n",
    "        return self.data_stream.sources\n",
    "\n",
    "    def transform_batch(self, batch):\n",
    "        batch_with_expanded_source = []\n",
    "        for i, (source, source_batch) in enumerate(\n",
    "                zip(self.data_stream.sources, batch)):\n",
    "            if source == 'source':\n",
    "#                 copy each source seqoyuence self.n_samples times, but keep the tensor 2d\n",
    "                \n",
    "                expanded_source = []\n",
    "                for ins in source_batch:\n",
    "                    expanded_source.extend([ins for _ in range(self.n_samples)])        \n",
    "                    \n",
    "                batch_with_expanded_source.append(expanded_source)\n",
    "            else:\n",
    "                batch_with_expanded_source.append(source_batch)\n",
    "                        \n",
    "        return tuple(batch_with_expanded_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: now call the main() function with the train and dev streams \n",
    "# TODO: test that the naive cost function works\n",
    "\n",
    " # Filter sequences that are too long\n",
    "# training_stream = Filter(training_stream,\n",
    "#                 predicate=_too_long(seq_len=exp_config['seq_len']))\n",
    "\n",
    "# Replace out of vocabulary tokens with unk token\n",
    "# training_stream = Mapping(training_stream,\n",
    "#                  _oov_to_unk(src_vocab_size=exp_config['src_vocab_size'],\n",
    "#                              trg_vocab_size=exp_config['trg_vocab_size'],\n",
    "#                              unk_id=exp_config['unk_id']))\n",
    "\n",
    "# Build a batched version of stream to read k batches ahead\n",
    "training_stream = Batch(training_stream,\n",
    "               iteration_scheme=ConstantScheme(\n",
    "                   exp_config['batch_size']*exp_config['sort_k_batches']))\n",
    "\n",
    "# Sort all samples in the read-ahead batch\n",
    "training_stream = Mapping(training_stream, SortMapping(_length))\n",
    "\n",
    "# Convert it into a stream again\n",
    "training_stream = Unpack(training_stream)\n",
    "\n",
    "# Construct batches from the stream with specified batch size\n",
    "training_stream = Batch(\n",
    "    training_stream, iteration_scheme=ConstantScheme(exp_config['batch_size']))\n",
    "\n",
    "# Pad sequences that are short\n",
    "# TODO: padding needs to be done over the flattened samples, or the Padding implementation needs to be\n",
    "# modified --\n",
    "# IDEA: add a transformer which flattens the target samples before we add the mask\n",
    "flat_sample_stream = FlattenSamples(training_stream)\n",
    "\n",
    "expanded_source_stream = CopySourceNTimes(flat_sample_stream, n_samples=5)\n",
    "\n",
    "# TODO: some sources can be excluded from the padding Op\n",
    "masked_stream = PaddingWithEOS(\n",
    "    expanded_source_stream, [exp_config['src_vocab_size'] - 1, exp_config['trg_vocab_size'] - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked_stream.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(encoder, decoder):\n",
    "\n",
    "    # Create Theano variables\n",
    "    logger.info('Creating theano variables')\n",
    "    source_sentence = tensor.lmatrix('source')\n",
    "    source_sentence_mask = tensor.matrix('source_mask')\n",
    "    \n",
    "#     target_samples = tensor.tensor3('samples').astype('int64')\n",
    "#     target_samples_mask = tensor.tensor3('target_samples_mask').astype('int64')\n",
    "    samples = tensor.lmatrix('samples')\n",
    "    samples_mask = tensor.matrix('samples_mask')\n",
    "    \n",
    "    # scores is (batch, samples)\n",
    "    scores = tensor.matrix('scores')\n",
    "    # We don't need a scores mask because there should be the same number of scores for each instance\n",
    "    # num samples is a hyperparameter of the model\n",
    "    \n",
    "    # the name is important to make sure pre-trained params get loaded correctly\n",
    "#     decoder.name = 'decoder'\n",
    "    \n",
    "    # This is the part that is different for the MinimumRiskSequenceGenerator\n",
    "    cost = decoder.expected_cost(\n",
    "        encoder.apply(source_sentence, source_sentence_mask),\n",
    "        source_sentence_mask, samples, samples_mask, scores)\n",
    "\n",
    "\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(model, cost, config, tr_stream, dev_stream, use_bokeh=False):\n",
    "    \n",
    "    logger.info(\"Loading parameters from model: {}\".format(config['saved_parameters']))\n",
    "    # load the parameter values from an .npz file\n",
    "    param_values = LoadNMT.load_parameter_values(config['saved_parameters'])\n",
    "    LoadNMT.set_model_parameters(model, param_values)\n",
    "    \n",
    "    logger.info('Creating computational graph')\n",
    "    cg = ComputationGraph(cost)\n",
    "    \n",
    "    # create the training directory, and copy this config there if directory doesn't exist\n",
    "#     if not os.path.isdir(config['saveto']):\n",
    "#         os.makedirs(config['saveto'])\n",
    "#         shutil.copy(config['config_file'], config['saveto'])\n",
    "\n",
    "    # Set extensions\n",
    "    logger.info(\"Initializing extensions\")\n",
    "    extensions = [\n",
    "        FinishAfter(after_n_batches=config['finish_after']),\n",
    "        TrainingDataMonitoring([cost], after_batch=True),\n",
    "        Printing(after_batch=True),\n",
    "#         CheckpointNMT(config['saveto'],\n",
    "#                       every_n_batches=config['save_freq'])\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Set up beam search and sampling computation graphs if necessary\n",
    "    \n",
    "#     if config['hook_samples'] >= 1 or config['bleu_script'] is not None:\n",
    "#         logger.info(\"Building sampling model\")\n",
    "#         sampling_representation = encoder.apply(\n",
    "#             sampling_input, tensor.ones(sampling_input.shape))\n",
    "#         # TODO: the generated output actually contains several more values, ipdb to see what they are\n",
    "#         generated = decoder.generate(sampling_input, sampling_representation)\n",
    "#         search_model = Model(generated)\n",
    "#         _, samples = VariableFilter(\n",
    "#             bricks=[decoder.sequence_generator], name=\"outputs\")(\n",
    "#                 ComputationGraph(generated[1]))  # generated[1] is next_outputs\n",
    "\n",
    "    # Add sampling\n",
    "#     if config['hook_samples'] >= 1:\n",
    "#         logger.info(\"Building sampler\")\n",
    "#         extensions.append(\n",
    "#             Sampler(model=search_model, data_stream=tr_stream,\n",
    "#                     hook_samples=config['hook_samples'],\n",
    "#                     every_n_batches=config['sampling_freq'],\n",
    "#                     src_vocab_size=config['src_vocab_size']))\n",
    "\n",
    "    # Add early stopping based on bleu\n",
    "#     if config['bleu_script'] is not None:\n",
    "#         logger.info(\"Building bleu validator\")\n",
    "#         extensions.append(\n",
    "#             BleuValidator(sampling_input, samples=samples, config=config,\n",
    "#                           model=search_model, data_stream=dev_stream,\n",
    "#                           normalize=config['normalized_bleu'],\n",
    "#                           every_n_batches=config['bleu_val_freq']))\n",
    "\n",
    "    # Reload model if necessary\n",
    "#     if config['reload']:\n",
    "#         extensions.append(LoadNMT(config['saveto']))\n",
    "\n",
    "    # Plot cost in bokeh if necessary\n",
    "#     if use_bokeh and BOKEH_AVAILABLE:\n",
    "#         extensions.append(\n",
    "#             Plot(config['model_save_directory'], channels=[['decoder_cost_cost'], ['validation_set_bleu_score']],\n",
    "#                  every_n_batches=10))\n",
    "\n",
    "    # Set up training algorithm\n",
    "    logger.info(\"Initializing training algorithm\")\n",
    "    # if there is dropout or random noise, we need to use the output of the modified graph\n",
    "#     if config['dropout'] < 1.0 or config['weight_noise_ff'] > 0.0:\n",
    "#         algorithm = GradientDescent(\n",
    "#             cost=cg.outputs[0], parameters=cg.parameters,\n",
    "#             step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "#                                      eval(config['step_rule'])()])\n",
    "#         )\n",
    "#     else:\n",
    "#         algorithm = GradientDescent(\n",
    "#             cost=cost, parameters=cg.parameters,\n",
    "#             step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "#                                      eval(config['step_rule'])()])\n",
    "#         )\n",
    "\n",
    "    algorithm = GradientDescent(\n",
    "        cost=cost, parameters=cg.parameters,\n",
    "        step_rule=CompositeRule([StepClipping(config['step_clipping']),\n",
    "                                 eval(config['step_rule'])()],\n",
    "                               ),\n",
    "        on_unused_sources='warn'\n",
    "    )\n",
    "\n",
    "    # enrich the logged information\n",
    "    extensions.append(\n",
    "        Timing(every_n_batches=100)\n",
    "    )\n",
    "\n",
    "    # Initialize main loop\n",
    "    logger.info(\"Initializing main loop\")\n",
    "    main_loop = MainLoop(\n",
    "        model=model,\n",
    "        algorithm=algorithm,\n",
    "        data_stream=tr_stream,\n",
    "        extensions=extensions\n",
    "    )\n",
    "\n",
    "    # Train!\n",
    "    main_loop.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_cost = create_model(train_encoder, train_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up training model\n",
    "logger.info(\"Building model\")\n",
    "train_model = Model(training_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_model.get_parameter_dict().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: create test functions and sanity check that probs does something reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masked_stream.sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_iter = masked_stream.get_epoch_iterator()\n",
    "\n",
    "source, source_mask, target, target_mask, samples, samples_mask, scores, scores_mask = test_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# logger.info('Creating theano variables')\n",
    "# source_sentence = tensor.lmatrix('source')\n",
    "# source_sentence_mask = tensor.matrix('source_mask')\n",
    "    \n",
    "#     target_samples = tensor.tensor3('samples').astype('int64')\n",
    "#     target_samples_mask = tensor.tensor3('target_samples_mask').astype('int64')\n",
    "# samples = tensor.lmatrix('samples')\n",
    "# samples_mask = tensor.matrix('samples_mask')\n",
    "    \n",
    "    # scores is (batch, samples)\n",
    "# scores = tensor.matrix('scores')\n",
    "\n",
    "test_func = train_model.get_theano_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = scores.astype('float32')\n",
    "out = test_func(scores, samples_mask, source_mask, source, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# numpy.exp(out)\n",
    "out[0].shape\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_ivocab = {v:k for k,v in src_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main(train_model, training_cost, exp_config, masked_stream, dev_stream=None, use_bokeh=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
